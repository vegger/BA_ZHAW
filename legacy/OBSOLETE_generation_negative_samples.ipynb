{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk import edit_distance\n",
    "import pandas as pd\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from transformers import BertForMaskedLM, BertTokenizer, pipeline, T5Tokenizer, T5EncoderModel\n",
    "import re\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "import json\n",
    "import matplotlib.patches as mpatches\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. take dataset \n",
    "2. take current row \n",
    "3. split it into: \n",
    "    - CDR3, V, J \n",
    "    - Epitope, MHC A, MHC B, MHC class \n",
    "4. take next (random) row (or for each)\n",
    "5. split it into: \n",
    "    - CDR3, V, J \n",
    "    - Epitope, MHC A, MHC B, MHC class\n",
    "6. compute levenshtein distance for: \n",
    "    - CDR3 region (row A to row B)\n",
    "    - Epitopes (row A to row B)\n",
    "7. if they are NOT to similar => search new row B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path_beta = \"../data/customDatasets/backup/beta_concatenated.tsv\"\n",
    "beta_df = pd.read_csv(read_path_beta, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = len(beta_df) - 1 \n",
    "negative_epitopes = []\n",
    "leven_threshold = 4 # this number is a magic number... => maybe change to random in reasonable intervall?\n",
    "# if we use a too high one only a few epitopes (the longer ones) can be potentially choosed. \n",
    "# This because only for the longer epitopes the levenshtein distance can then be matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_negative_epitope(df, index):\n",
    "    df = df\n",
    "    index = index\n",
    "    epitope = df[\"Epitope\"][index]\n",
    "    # print(epitope)\n",
    "    random_epitope_index = np.random.randint(0, max_index)\n",
    "    random_epitope = df[\"Epitope\"][random_epitope_index]\n",
    "    leven_dist = edit_distance(epitope, random_epitope)\n",
    "\n",
    "    if(leven_dist >= leven_threshold): \n",
    "        negative_epitopes.append(random_epitope)\n",
    "    else: \n",
    "        search_negative_epitope(df, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes some minutes, thats why it is commented out! If needed, remove the comments!\n",
    "'''\n",
    "for i, epitope in enumerate(beta_df[\"Epitope\"]): \n",
    "    search_negative_epitope(beta_df, i)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_epitopes_df = pd.DataFrame(negative_epitopes, columns=[\"Negative Epitope\"])\n",
    "negative_epitopes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I (CG) think that this is a bit hacky, this beacuase we compare sequences with different length and the Levenshtein distance is designed to compare strings from the same length...\n",
    "\n",
    "Idea: create embedding of epitope and measure similarity of them or cluster them..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output dimensions from [ProtBert on HuggingFace](https://huggingface.co/Rostlab/prot_bert) seems wrong in my opinion... => not wrong but somehow not usable directly as a representation. After elaborating i saw that in the [GitHub Repo of the ProtTrans](https://github.com/agemagician/ProtTrans/tree/master?tab=readme-ov-file#models) they promote another, better performing model called ProtT5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load encoder-part of ProtT5 in half-precision. { display-mode: \"form\" }\n",
    "# Load ProtT5 in half-precision (more specifically: the encoder-part of ProtT5-XL-U50 in half-precision)\n",
    "transformer_link = \"Rostlab/prot_t5_xl_half_uniref50-enc\"\n",
    "print(\"Loading: {}\".format(transformer_link))\n",
    "model = T5EncoderModel.from_pretrained(transformer_link)\n",
    "if device==torch.device(\"cpu\"):\n",
    "  print(\"Casting model to full precision for running on CPU ...\")\n",
    "  model.to(torch.float32) # only cast to full-precision if no GPU is available\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "tokenizer = T5Tokenizer.from_pretrained(transformer_link, do_lower_case=False, legacy=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONSIDER THIS:\n",
    "Maybe instead of just passing the Epitope we should also pass the MHC information available.. This because in my opinion if I pass the same epitope I should theoretically always get the same embedding, right? => After consideration: This is in my opinion not feasible as the PLMs only want AA sequences as input (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epitopes = set(beta_df[\"Epitope\"].to_list())\n",
    "len(epitopes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will replace all rare/ambiguous amino acids by X and introduce white-space between all amino acids\n",
    "processed_epitopes = [(sequence, \" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence)))) for sequence in epitopes]\n",
    "# processed_epitopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(processed_seqs):\n",
    "    # Extract just the processed sequences for tokenization\n",
    "    sequences = [seq[1] for seq in processed_seqs]\n",
    "    ids = tokenizer.batch_encode_plus(sequences, add_special_tokens=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "    input_ids = ids['input_ids'].to(device)\n",
    "    attention_mask = ids['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    \n",
    "    # Now, return embeddings mapped to the original sequence\n",
    "    embeddings = {}\n",
    "    for i, (original_seq, _) in enumerate(processed_seqs):\n",
    "        seq_len = attention_mask[i].sum().item() - 2  # Subtract [CLS] and [SEP]\n",
    "        valid_embeddings = last_hidden_states[i, 1:seq_len+1]\n",
    "        mean_embedding = valid_embeddings.mean(dim=0)\n",
    "        embeddings[original_seq] = mean_embedding.cpu().numpy()  # Use original sequence as key\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 256\n",
    "sequence_to_embedding = {}\n",
    "\n",
    "# Batch processing with a dictionary, using original sequences as keys\n",
    "for i in range(0, len(processed_epitopes), batch_size):\n",
    "    batch_sequences = processed_epitopes[i:i+batch_size]\n",
    "    batch_embeddings = process_batch(batch_sequences)\n",
    "    sequence_to_embedding.update(batch_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "beta_df[\"Epitope Embedding\"] = beta_df[\"Epitope\"].map(sequence_to_embedding)\n",
    "beta_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is needed becuase the embedding is huge and otherwise it would be stored with line breaks (\\n) \n",
    "# This would make it difficult while reading the file\n",
    "beta_df['Epitope Embedding'] = beta_df['Epitope Embedding'].apply(lambda x: json.dumps(x.tolist()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_path = \"../data/customDatasets/negative_samples/temp\"\n",
    "file_name = \"beta_concatenated_with_epitope_embedding.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beta_df.to_csv(to_path+\"/\"+file_name, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_df = pd.read_csv(to_path+\"/\"+file_name, sep=\"\\t\")\n",
    "beta_df['Epitope Embedding'] = beta_df['Epitope Embedding'].apply(lambda x: np.array(json.loads(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for i, embedding in enumerate(beta_df[\"Epitope Embedding\"]): \n",
    "    # print(str(embedding.tolist()))\n",
    "    embeddings.append(str(embedding.tolist()))\n",
    "\n",
    "len(set(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(beta_df[\"Epitope\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Similarity Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(embedding1, embedding2): \n",
    "    cosine = np.dot(embedding1,embedding2)/(np.linalg.norm(embedding1)*np.linalg.norm(embedding2))\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = len(beta_df) - 1 \n",
    "negative_samples_cosine = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_negative(cosine_similarity, current_epitope, random_epitope): \n",
    "    is_valid = False\n",
    "    cosine_threshold = np.random.uniform(-0.5, 0.5)\n",
    "\n",
    "    if (cosine_similarity <= cosine_threshold) \\\n",
    "        and (current_epitope != random_epitope): \n",
    "        is_valid = True \n",
    "\n",
    "    return is_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_negative_epitope_embedding(df, index, current_epitope):\n",
    "    df = df\n",
    "    index = index\n",
    "    epitope = current_epitope\n",
    "    embedding = df[\"Epitope Embedding\"][index]\n",
    "    # print(epitope_embedding)\n",
    "    random_epitope_index = np.random.randint(0, max_index)\n",
    "    random_epitope = df[\"Epitope\"][random_epitope_index]\n",
    "    random_epitope_embedding = df[\"Epitope Embedding\"][random_epitope_index]\n",
    "    cosine = cosine_similarity(embedding, random_epitope_embedding)\n",
    "\n",
    "    if is_valid_negative(cosine, epitope, random_epitope): \n",
    "        negative_samples_cosine.append(random_epitope)\n",
    "    else: \n",
    "        search_negative_epitope(df, index)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes some minutes, thats why it is commented out! If needed, remove the comments!\n",
    "\n",
    "for i, epitope in enumerate(beta_df[\"Epitope\"]): \n",
    "    search_negative_epitope_embedding(beta_df, i, epitope)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_samples_cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Appraoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, with UMAP consider a good value for n_components...\n",
    "=> if we go higher than 2, we need to make 2 embedding lists, one for the clustering (x dimensions) and one for the visualization (2 dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epitopes_list = (epitopes)\n",
    "# here use a set of the embeddings because many duplicates as same epitopes\n",
    "# Do so becuase very slow if every embedding is processed \n",
    "umap_embeddings = umap.UMAP(\n",
    "    n_neighbors=30,\n",
    "    min_dist=0.0,\n",
    "    n_components=2,\n",
    "    random_state=42,\n",
    ").fit_transform(epitopes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(umap_embeddings, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMAP creates unique embedding for each row\n",
    "\n",
    "Before there was an equal number of Epitope <=> Embeddings. After the UMAP processing we have for each Epitope an unique Embedding... I Think this is normal becuase o the modification UMAP does (as I understood UMAP tries to perserves general information so \"overall\") but somehow strange to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Epitope Embeddings Visualization\")\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1], s=0.1, cmap='Spectral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdb = HDBSCAN(\n",
    "    min_cluster_size=5, \n",
    "    min_samples=5)\n",
    "hdb.fit(umap_embeddings)\n",
    "\n",
    "labels = hdb.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered = (labels >= 0)\n",
    "\n",
    "# Create a list of patches for the legend\n",
    "clusters = np.unique(labels)\n",
    "legend_patches = [\n",
    "    mpatches.Patch(color=plt.cm.Spectral(label / np.max(clusters)), label=f'Cluster {label}')\n",
    "    for label in clusters if label >= 0\n",
    "]\n",
    "legend_patches.insert(0, mpatches.Patch(color='gray', label='Noise'))\n",
    "\n",
    "# Define the figure size and adjust it as necessary\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot the scatter points\n",
    "plt.scatter(umap_embeddings[~clustered, 0], umap_embeddings[~clustered, 1],\n",
    "            color='gray', s=1, alpha=0.5, label='Noise')\n",
    "plt.scatter(umap_embeddings[clustered, 0], umap_embeddings[clustered, 1],\n",
    "            c=labels[clustered], s=1, cmap='Spectral')\n",
    "\n",
    "# Add a legend with a specified number of columns\n",
    "num_legend_cols = 6\n",
    "plt.legend(handles=legend_patches, loc='upper center', bbox_to_anchor=(0.5, -0.05),\n",
    "           fancybox=True, shadow=True, ncol=num_legend_cols)\n",
    "\n",
    "# Adjust the layout to make space for the legend\n",
    "plt.tight_layout(rect=[0, 0.1, 1, 1])  # The rect argument adjusts the subplot position\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame that contains your UMAP components and labels\n",
    "df = pd.DataFrame(umap_embeddings, columns=['UMAP 1', 'UMAP 2'])\n",
    "df['Label'] = labels\n",
    "df['Epitope'] = beta_df[\"Epitope\"]  # Replace with your actual epitope data\n",
    "\n",
    "# Create the figure using Plotly Express\n",
    "fig = px.scatter(\n",
    "    df, x='UMAP 1', y='UMAP 2',\n",
    "    color='Label',  # This will use your labels for coloring\n",
    "    hover_data=['Epitope'],  # This will show the epitope on hover\n",
    "    color_continuous_scale=px.colors.sequential.Viridis,  # Optional color scale for aesthetic purposes\n",
    "    labels={'Label': 'Cluster Label'},  # Rename legends and axes\n",
    ")\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_layout(\n",
    "    title='Epitope Clustering Visualization',\n",
    "    coloraxis_colorbar=dict(title='Cluster Label'),\n",
    "    hoverlabel=dict(bgcolor=\"white\", font_size=12, font_family=\"Rockwell\"),\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beta_df[\"HDBSCAN Label\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beta_df[\"HDBSCAN Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = len(beta_df) - 1 \n",
    "negative_samples_clustering1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_negative_epitope_clustering1(df, index, current_epitope):\n",
    "    df = df\n",
    "    index = index\n",
    "    epitope = current_epitope\n",
    "    label = df[\"HDBSCAN Label\"][index]\n",
    "    # print(epitope_embedding)\n",
    "    random_epitope_index = np.random.randint(0, max_index)\n",
    "    random_epitope = df[\"Epitope\"][random_epitope_index]\n",
    "    random_epitope_label = df[\"HDBSCAN Label\"][random_epitope_index]\n",
    "\n",
    "    if random_epitope_label != label and \\\n",
    "        random_epitope != epitope: \n",
    "        negative_samples_clustering1.append(random_epitope)\n",
    "    else: \n",
    "        search_negative_epitope(df, index)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes some minutes, thats why it is commented out! If needed, remove the comments!\n",
    "\n",
    "for i, epitope in enumerate(beta_df[\"Epitope\"]): \n",
    "    search_negative_epitope_clustering1(beta_df, i, epitope)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP + HDBSCAN\n",
    "negative_samples_clustering1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "to_path = \"../data/customDatasets/negative_samples/temp\"\n",
    "file_name = \"beta_epitope_embeddings.tsv\"\n",
    "beta_df.to_csv(to_path+\"/\"+file_name, sep=\"\\t\", index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use PCA as well for completeness\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_embeddings = pca.fit_transform(beta_df[\"Epitope Embedding\"].to_list())\n",
    "len(np.unique(pca_embeddings, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"PCA - Epitope Embeddings Visualization\")\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.scatter(pca_embeddings[:, 0], pca_embeddings[:, 1], s=0.1, cmap='Spectral');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "very low explained variance with n_components = 2!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "criterias to be a nagative sample for an epitope: \n",
    "1. check if epitope is in same cluster \n",
    "2. if not: take this epitope as negative sample \n",
    "\n",
    "=> I (CG) do not highly trust the clustering... maybe we can combine it with the levenshtein approach? so: \n",
    "\n",
    "    if not in cluster AND levenshtein >= x, where x is a random number in an interval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BA24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
