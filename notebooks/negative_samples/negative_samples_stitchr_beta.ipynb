{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline, T5Tokenizer, T5EncoderModel\n",
    "import torch\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Samples Creation w/ Embeddings and Coisne Similarity\n",
    "\n",
    "Here we chose the approach to create embeddings from the prot_t5_xl model and then check if the embedding is in a certain \"un-similarity range\" which is a self-defined threshold. This becuase evaluations showed that this is (at least as far as we consider) the \"best\" approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path_beta = \"../../data/customDatasets/Stitchr_beta_concatenated.tsv\"\n",
    "stitchr_beta_df = pd.read_csv(read_path_beta, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitchr_beta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(stitchr_beta_df[\"Epitope\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitchr_beta_df['Epitope'] = stitchr_beta_df['Epitope'].astype(str)\n",
    "epitope_counts = stitchr_beta_df['Epitope'].value_counts().reset_index()\n",
    "print(epitope_counts)\n",
    "epitope_counts.columns = ['Epitope Name', 'Count'] \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(stitchr_beta_df['Epitope'], bins=len(epitope_counts), edgecolor='k', alpha=0.7)\n",
    "plt.xlabel('Epitope Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Epitope Data')\n",
    "plt.xticks([])  # removes the x-axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(epitope_counts.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device: {}\".format(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Load encoder-part of ProtT5 in half-precision. { display-mode: \"form\" }\n",
    "# Load ProtT5 in half-precision (more specifically: the encoder-part of ProtT5-XL-U50 in half-precision)\n",
    "transformer_link = \"Rostlab/prot_t5_xl_half_uniref50-enc\"\n",
    "print(\"Loading: {}\".format(transformer_link))\n",
    "model = T5EncoderModel.from_pretrained(transformer_link)\n",
    "if device==torch.device(\"cpu\"):\n",
    "  print(\"Casting model to full precision for running on CPU ...\")\n",
    "  model.to(torch.float32) # only cast to full-precision if no GPU is available\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "tokenizer = T5Tokenizer.from_pretrained(transformer_link, do_lower_case=False, legacy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epitopes = set(stitchr_beta_df[\"Epitope\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will replace all rare/ambiguous amino acids by X and introduce white-space between all amino acids\n",
    "processed_epitopes = [(sequence, \" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence)))) for sequence in epitopes]\n",
    "# processed_epitopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(processed_seqs):\n",
    "    # Extract just the processed sequences for tokenization\n",
    "    sequences = [seq[1] for seq in processed_seqs]\n",
    "    ids = tokenizer.batch_encode_plus(sequences, add_special_tokens=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "    input_ids = ids['input_ids'].to(device)\n",
    "    attention_mask = ids['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    \n",
    "    # Now, return embeddings mapped to the original sequence\n",
    "    embeddings = {}\n",
    "    for i, (original_seq, _) in enumerate(processed_seqs):\n",
    "        seq_len = attention_mask[i].sum().item() - 2  # Subtract [CLS] and [SEP]\n",
    "        valid_embeddings = last_hidden_states[i, 1:seq_len+1]\n",
    "        mean_embedding = valid_embeddings.mean(dim=0)\n",
    "        embeddings[original_seq] = mean_embedding.cpu().numpy()  # Use original sequence as key\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_path = \"../../data/customDatasets/negative_samples/temp\"\n",
    "file_name = \"Stitchr_beta_concatenated_with_epitope_embedding.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 128\n",
    "sequence_to_embedding = {}\n",
    "\n",
    "# Batch processing with a dictionary, using original sequences as keys\n",
    "for i in range(0, len(processed_epitopes), batch_size):\n",
    "    batch_sequences = processed_epitopes[i:i+batch_size]\n",
    "    batch_embeddings = process_batch(batch_sequences)\n",
    "    sequence_to_embedding.update(batch_embeddings)\n",
    "\n",
    "    stitchr_beta_df[\"Epitope Embedding\"] = stitchr_beta_df[\"Epitope\"].map(sequence_to_embedding)\n",
    "\n",
    "# This is needed becuase the embedding is huge and otherwise it would be stored with line breaks (\\n) \n",
    "# This would make it difficult while reading the file\n",
    "stitchr_beta_df['Epitope Embedding'] = stitchr_beta_df['Epitope Embedding'].apply(lambda x: json.dumps(x.tolist()))\n",
    "\n",
    "stitchr_beta_df.to_csv(to_path+\"/\"+file_name, sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above is commented out to safe time. After changing something in the underlaying dataset re-run this cell to create the up-to-date embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stitchr_beta_df = pd.read_csv(to_path+\"/\"+file_name, sep=\"\\t\")\n",
    "stitchr_beta_df['Epitope Embedding'] = stitchr_beta_df['Epitope Embedding'].apply(lambda x: np.array(json.loads(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = len(stitchr_beta_df) - 1 \n",
    "negative_epitopes_cosine = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(embedding1, embedding2): \n",
    "    cosine = np.dot(embedding1,embedding2)/(np.linalg.norm(embedding1)*np.linalg.norm(embedding2))\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_negative(cosine_similarity, current_epitope, random_epitope): \n",
    "    is_valid = False\n",
    "    cosine_min = -1\n",
    "    cosine_max = 0.75\n",
    "\n",
    "    if (cosine_similarity >= cosine_min \\\n",
    "        and cosine_similarity <= cosine_max) \\\n",
    "        and (current_epitope != random_epitope): \n",
    "        is_valid = True \n",
    "\n",
    "    return is_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_max_depth = sys.getrecursionlimit()\n",
    "max_attempts_by_system = sys_max_depth - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stitchr_beta_df[\"Epitope\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_negative_epitope_embedding(df, index, current_epitope, max_attempts=max_attempts_by_system):\n",
    "    current_embedding = df[\"Epitope Embedding\"][index]\n",
    "    attempt = 0\n",
    "    \n",
    "    while attempt < max_attempts:\n",
    "        random_epitope_index = np.random.randint(0, len(df))\n",
    "        random_epitope = df[\"Epitope\"][random_epitope_index]\n",
    "        \n",
    "        if random_epitope_index == index:\n",
    "            attempt += 1\n",
    "            continue  # Skip the rest of the loop and try again\n",
    "        \n",
    "        random_epitope_embedding = df[\"Epitope Embedding\"][random_epitope_index]\n",
    "        cosine = cosine_similarity(current_embedding, random_epitope_embedding)\n",
    "        \n",
    "        if is_valid_negative(cosine, current_epitope, random_epitope) or attempt == max_attempts - 1:\n",
    "            return random_epitope  # Return the found valid or last attempt epitope\n",
    "        \n",
    "        attempt += 1\n",
    "    \n",
    "    # This point should theoretically never be reached because of the check in the loop,\n",
    "    # but it's a fallback to return a random different epitope if for some reason it does.\n",
    "    while True:\n",
    "        random_epitope_index = np.random.randint(0, len(df))\n",
    "        if random_epitope_index != index:\n",
    "            return df[\"Epitope\"][random_epitope_index]\n",
    "\n",
    "for i, epitope in enumerate(stitchr_beta_df[\"Epitope\"]):\n",
    "    negative_epitope = search_negative_epitope_embedding(stitchr_beta_df, i, epitope)\n",
    "    negative_epitopes_cosine.append(negative_epitope)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len((negative_epitopes_cosine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_epitopes_cosine_dict = {\"Negative Epitope\": negative_epitopes_cosine}\n",
    "negative_epitopes_cosine_df = pd.DataFrame(negative_epitopes_cosine_dict)\n",
    "# print(negative_epitopes_cosine_df.to_string())\n",
    "negative_epitopes_cosine_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epitope_counts_negative = negative_epitopes_cosine_df['Negative Epitope'].value_counts().reset_index()\n",
    "epitope_counts_negative.columns = ['Epitope Name', 'Count']\n",
    "print(epitope_counts_negative) \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(negative_epitopes_cosine_df['Negative Epitope'].astype(str), bins=len(epitope_counts_negative), edgecolor='k', alpha=0.7)\n",
    "plt.xlabel('Epitope Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Negative Epitope Data')\n",
    "plt.xticks([])  # removes the x-axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitchr_beta_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitchr_beta_with_negative_df = stitchr_beta_df.drop([\"Epitope Embedding\"], axis=1).copy(deep=True)\n",
    "stitchr_beta_with_negative_df[\"Binding\"] = 0\n",
    "stitchr_beta_with_negative_df[\"Epitope\"] = negative_epitopes_cosine\n",
    "stitchr_beta_with_negative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitchr_beta_with_negative_df = pd.concat([stitchr_beta_df.drop([\"Epitope Embedding\"], axis=1).copy(deep=True), stitchr_beta_with_negative_df], axis=0)\n",
    "stitchr_beta_with_negative_df[\"TCR_name\"] = range(1, len(stitchr_beta_with_negative_df)+1)\n",
    "stitchr_beta_with_negative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_path = \"../../data/customDatasets/negative_samples/\"\n",
    "file_name = \"Stitchr_beta_concatenated_with_negative.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitchr_beta_with_negative_df.to_csv(to_path+\"/\"+file_name, sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BA24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
