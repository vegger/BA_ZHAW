{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline, T5Tokenizer, T5EncoderModel\n",
    "import torch\n",
    "import re\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Samples Creation w/ Embeddings and Coisne Similarity\n",
    "\n",
    "Here we chose the approach to create embeddings from the prot_t5_xl model and then check if the embedding is in a certain \"un-similarity range\" which is a self-defined threshold. This becuase evaluations showed that this is (at least as far as we consider) the \"best\" approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path_beta = \"../../data/customDatasets/paired_concatenated.tsv\"\n",
    "paired_df = pd.read_csv(read_path_beta, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(paired_df[\"Epitope\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epitope_counts = paired_df['Epitope'].value_counts().reset_index()\n",
    "print(epitope_counts)\n",
    "epitope_counts.columns = ['Epitope Name', 'Count'] \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(paired_df['Epitope'], bins=len(epitope_counts), edgecolor='k', alpha=0.7)\n",
    "plt.xlabel('Epitope Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Epitope Data')\n",
    "plt.xticks([])  # removes the x-axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device: {}\".format(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Load encoder-part of ProtT5 in half-precision. { display-mode: \"form\" }\n",
    "# Load ProtT5 in half-precision (more specifically: the encoder-part of ProtT5-XL-U50 in half-precision)\n",
    "transformer_link = \"Rostlab/prot_t5_xl_half_uniref50-enc\"\n",
    "print(\"Loading: {}\".format(transformer_link))\n",
    "model = T5EncoderModel.from_pretrained(transformer_link)\n",
    "if device==torch.device(\"cpu\"):\n",
    "  print(\"Casting model to full precision for running on CPU ...\")\n",
    "  model.to(torch.float32) # only cast to full-precision if no GPU is available\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "tokenizer = T5Tokenizer.from_pretrained(transformer_link, do_lower_case=False, legacy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epitopes = set(paired_df[\"Epitope\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will replace all rare/ambiguous amino acids by X and introduce white-space between all amino acids\n",
    "processed_epitopes = [(sequence, \" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence)))) for sequence in epitopes]\n",
    "# processed_epitopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(processed_seqs):\n",
    "    # Extract just the processed sequences for tokenization\n",
    "    sequences = [seq[1] for seq in processed_seqs]\n",
    "    ids = tokenizer.batch_encode_plus(sequences, add_special_tokens=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "    input_ids = ids['input_ids'].to(device)\n",
    "    attention_mask = ids['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    \n",
    "    # Now, return embeddings mapped to the original sequence\n",
    "    embeddings = {}\n",
    "    for i, (original_seq, _) in enumerate(processed_seqs):\n",
    "        seq_len = len(original_seq)\n",
    "        valid_embeddings = last_hidden_states[i,:seq_len]\n",
    "        per_protein_embedding = valid_embeddings.mean(dim=0)\n",
    "        # print(f\"mean_embedding.shape: {per_protein_embedding.shape}\")\n",
    "        embeddings[original_seq] = per_protein_embedding.cpu().numpy()  # Use original sequence as key\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_path = \"../../data/customDatasets/negative_samples/temp\"\n",
    "file_name = \"Stitchr_paired_concatenated_with_epitope_embedding.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 128\n",
    "sequence_to_embedding = {}\n",
    "\n",
    "# Batch processing with a dictionary, using original sequences as keys\n",
    "for i in range(0, len(processed_epitopes), batch_size):\n",
    "    batch_sequences = processed_epitopes[i:i+batch_size]\n",
    "    batch_embeddings = process_batch(batch_sequences)\n",
    "    sequence_to_embedding.update(batch_embeddings)\n",
    "\n",
    "    paired_df[\"Epitope Embedding\"] = paired_df[\"Epitope\"].map(sequence_to_embedding)\n",
    "\n",
    "# This is needed becuase the embedding is huge and otherwise it would be stored with line breaks (\\n) \n",
    "# This would make it difficult while reading the file\n",
    "paired_df['Epitope Embedding'] = paired_df['Epitope Embedding'].apply(lambda x: json.dumps(x.tolist()))\n",
    "\n",
    "paired_df.to_csv(to_path+\"/\"+file_name, sep=\"\\t\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above is commented out to safe time. After changing something in the underlaying dataset re-run this cell to create the up-to-date embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paired_df = pd.read_csv(to_path+\"/\"+file_name, sep=\"\\t\")\n",
    "paired_df['Epitope Embedding'] = paired_df['Epitope Embedding'].apply(lambda x: np.array(json.loads(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = len(paired_df) - 1 \n",
    "negative_epitopes_cosine = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(embedding1, embedding2): \n",
    "    cosine = np.dot(embedding1,embedding2)/(np.linalg.norm(embedding1)*np.linalg.norm(embedding2))\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_negative(cosine_similarity, current_epitope, random_epitope): \n",
    "    is_valid = False\n",
    "    cosine_min = -1\n",
    "    cosine_max = 0.75\n",
    "\n",
    "    if (cosine_similarity >= cosine_min \\\n",
    "        and cosine_similarity <= cosine_max) \\\n",
    "        and (current_epitope != random_epitope): \n",
    "        is_valid = True \n",
    "\n",
    "    return is_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_negative_epitope_embedding(df, index, current_epitope):\n",
    "    df = df\n",
    "    index = index\n",
    "    epitope = current_epitope\n",
    "    embedding = df[\"Epitope Embedding\"][index]\n",
    "    # print(epitope_embedding)\n",
    "    random_epitope_index = np.random.randint(0, max_index)\n",
    "    random_epitope = df[\"Epitope\"][random_epitope_index]\n",
    "    random_epitope_embedding = df[\"Epitope Embedding\"][random_epitope_index]\n",
    "    random_mhc_a = df[\"MHC A\"][random_epitope_index]\n",
    "    random_mhc_b = df[\"MHC B\"][random_epitope_index]\n",
    "    cosine = cosine_similarity(embedding, random_epitope_embedding)\n",
    "\n",
    "    if is_valid_negative(cosine, epitope, random_epitope): \n",
    "        negative_epitopes_cosine.append((random_epitope, random_mhc_a, random_mhc_b))\n",
    "    else: \n",
    "        search_negative_epitope_embedding(df, index, current_epitope)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, epitope in enumerate(paired_df[\"Epitope\"]): \n",
    "    search_negative_epitope_embedding(paired_df, i, epitope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len((negative_epitopes_cosine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epitopes = []\n",
    "mhc_a = []\n",
    "mhc_b = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row_infos in negative_epitopes_cosine:\n",
    "    epitopes.append(row_infos[0]) \n",
    "    mhc_a.append(row_infos[1])\n",
    "    mhc_b.append(row_infos[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_epitopes_cosine_dict = {\"Negative Epitope\": epitopes, \"MHC A\": mhc_a, \"MHC B\": mhc_b}\n",
    "negative_epitopes_cosine_df = pd.DataFrame(negative_epitopes_cosine_dict)\n",
    "# print(negative_epitopes_cosine_df.to_string())\n",
    "negative_epitopes_cosine_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epitope_counts_negative = negative_epitopes_cosine_df['Negative Epitope'].value_counts().reset_index()\n",
    "epitope_counts_negative.columns = ['Epitope Name', 'Count']\n",
    "print(epitope_counts_negative) \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(negative_epitopes_cosine_df['Negative Epitope'].astype(str), bins=len(epitope_counts_negative), edgecolor='k', alpha=0.7)\n",
    "plt.xlabel('Epitope Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Negative Epitope Data')\n",
    "plt.xticks([])  # removes the x-axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_with_negative_df = paired_df.drop([\"Epitope Embedding\", \"MHC A\", \"MHC B\"], axis=1).copy(deep=True)\n",
    "paired_with_negative_df[\"Epitope\"] = negative_epitopes_cosine\n",
    "paired_with_negative_df[\"MHC A\"] = mhc_a\n",
    "paired_with_negative_df[\"MHC B\"] = mhc_b\n",
    "paired_with_negative_df[\"Binding\"] = 0\n",
    "paired_with_negative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_with_negative_df = pd.concat([paired_df.drop([\"Epitope Embedding\"], axis=1).copy(deep=True), paired_with_negative_df], axis=0)\n",
    "paired_with_negative_df[\"TCR_name\"] = range(1, len(paired_with_negative_df)+1)\n",
    "paired_with_negative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_path = \"../../data/customDatasets/negative_samples/\"\n",
    "file_name = \"paired_concatenated_with_negative.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_with_negative_df.to_csv(to_path+\"/\"+file_name, sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BA24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
