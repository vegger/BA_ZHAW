{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline, T5Tokenizer, T5EncoderModel\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Samples Creation w/ Embeddings and Coisne Similarity\n",
    "\n",
    "Here we chose the approach to create embeddings from the prot_t5_xl model and then check if the embedding is in a certain \"un-similarity range\" which is a self-defined threshold. This becuase evaluations showed that this is (at least as far as we consider) the \"best\" approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path = \"../../data/customDatasets/paired_concatenated.tsv\"\n",
    "paired_df = pd.read_csv(read_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(paired_df[\"Epitope\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Load encoder-part of ProtT5 in half-precision. { display-mode: \"form\" }\n",
    "# Load ProtT5 in half-precision (more specifically: the encoder-part of ProtT5-XL-U50 in half-precision)\n",
    "transformer_link = \"Rostlab/prot_t5_xl_half_uniref50-enc\"\n",
    "print(\"Loading: {}\".format(transformer_link))\n",
    "model = T5EncoderModel.from_pretrained(transformer_link)\n",
    "if device==torch.device(\"cpu\"):\n",
    "  print(\"Casting model to full precision for running on CPU ...\")\n",
    "  model.to(torch.float32) # only cast to full-precision if no GPU is available\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "tokenizer = T5Tokenizer.from_pretrained(transformer_link, do_lower_case=False, legacy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epitopes = set(paired_df[\"Epitope\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will replace all rare/ambiguous amino acids by X and introduce white-space between all amino acids\n",
    "processed_epitopes = [(sequence, \" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence)))) for sequence in epitopes]\n",
    "# processed_epitopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(processed_seqs):\n",
    "    # Extract just the processed sequences for tokenization\n",
    "    sequences = [seq[1] for seq in processed_seqs]\n",
    "    ids = tokenizer.batch_encode_plus(sequences, add_special_tokens=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "    input_ids = ids['input_ids'].to(device)\n",
    "    attention_mask = ids['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    \n",
    "    # Now, return embeddings mapped to the original sequence\n",
    "    embeddings = {}\n",
    "    for i, (original_seq, _) in enumerate(processed_seqs):\n",
    "        seq_len = len(original_seq)\n",
    "        valid_embeddings = last_hidden_states[i,:seq_len]\n",
    "        per_protein_embedding = valid_embeddings.mean(dim=0)        \n",
    "        embedding = per_protein_embedding.cpu().numpy()\n",
    "        embeddings[original_seq] = embedding  # Use original sequence as key\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "to_path = \"../../data/customDatasets/negative_samples/temp/\"\n",
    "file_name = \"negative_samples_paired_embeddings_dict.npz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 128\n",
    "sequence_to_embedding = {}\n",
    "\n",
    "# Batch processing with a dictionary, using original sequences as keys\n",
    "for i in range(0, len(processed_epitopes), batch_size):\n",
    "    batch_sequences = processed_epitopes[i:i+batch_size]\n",
    "    batch_embeddings = process_batch(batch_sequences)\n",
    "    sequence_to_embedding.update(batch_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(to_path+file_name, **sequence_to_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epitope_to_embedding = np.load(to_path+file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = len(paired_df) - 1 \n",
    "negative_epitopes_cosine = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(embedding1, embedding2): \n",
    "    cosine = np.dot(embedding1,embedding2)/(np.linalg.norm(embedding1)*np.linalg.norm(embedding2))\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_negative(cosine_similarity, current_epitope, random_epitope): \n",
    "    is_valid = False\n",
    "    cosine_min = -1\n",
    "    cosine_max = 0.75\n",
    "\n",
    "    if (cosine_similarity >= cosine_min \\\n",
    "        and cosine_similarity <= cosine_max) \\\n",
    "        and (current_epitope != random_epitope): \n",
    "        is_valid = True \n",
    "\n",
    "    return is_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_max_depth = sys.getrecursionlimit()\n",
    "max_attempts_by_system = sys_max_depth - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_negative_epitope_embedding(df, index, current_epitope, max_attempts=max_attempts_by_system): \n",
    "    current_epitope = df[\"Epitope\"][index]\n",
    "    current_embedding = epitope_to_embedding[current_epitope]\n",
    "    attempt = 0\n",
    "    \n",
    "    while attempt < max_attempts:\n",
    "        random_epitope_index = np.random.randint(0, len(df))\n",
    "        random_epitope = df[\"Epitope\"][random_epitope_index]\n",
    "        random_mhc_a = df[\"MHC A\"][random_epitope_index]\n",
    "        random_mhc_b = df[\"MHC B\"][random_epitope_index]\n",
    "        \n",
    "        if random_epitope_index == index:\n",
    "            attempt += 1\n",
    "            continue  # Skip the rest of the loop and try again\n",
    "        \n",
    "        random_epitope_embedding = epitope_to_embedding[random_epitope]\n",
    "        cosine = cosine_similarity(current_embedding, random_epitope_embedding)\n",
    "        \n",
    "        if is_valid_negative(cosine, current_epitope, random_epitope) or attempt == max_attempts - 1:\n",
    "            return (random_epitope, random_mhc_a, random_mhc_b)  # Return the found valid or last attempt epitope\n",
    "        \n",
    "        attempt += 1\n",
    "    \n",
    "    # This point should theoretically never be reached because of the check in the loop,\n",
    "    # but it's a fallback to return a random different epitope if for some reason it does.\n",
    "    while True:\n",
    "        random_epitope_index = np.random.randint(0, len(df))\n",
    "        if random_epitope_index != index:\n",
    "            return df[\"Epitope\"][random_epitope_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, epitope in enumerate(paired_df[\"Epitope\"]):\n",
    "    negative_epitope = search_negative_epitope_embedding(paired_df, i, epitope)\n",
    "    negative_epitopes_cosine.append(negative_epitope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(negative_epitopes_cosine)) # should be: 56'833"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epitopes = []\n",
    "mhc_a = []\n",
    "mhc_b = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row_infos in negative_epitopes_cosine:\n",
    "    epitopes.append(row_infos[0]) \n",
    "    mhc_a.append(row_infos[1])\n",
    "    mhc_b.append(row_infos[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_epitopes_cosine_dict = {\"Negative Epitope\": epitopes, \"MHC A\": mhc_a, \"MHC B\": mhc_b}\n",
    "negative_epitopes_cosine_df = pd.DataFrame(negative_epitopes_cosine_dict)\n",
    "# print(negative_epitopes_cosine_df.to_string())\n",
    "# print(negative_epitopes_cosine_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_epitopes_cosine_df[\"Negative Epitope\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_negative_epitope_df = paired_df.drop([\"MHC A\", \"MHC B\"], axis=1).copy(deep=True)\n",
    "paired_negative_epitope_df[\"Epitope\"] = epitopes\n",
    "paired_negative_epitope_df[\"MHC A\"] = mhc_a\n",
    "paired_negative_epitope_df[\"MHC B\"] = mhc_b\n",
    "paired_negative_epitope_df[\"Binding\"] = 0\n",
    "paired_negative_epitope_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_with_negative_df = pd.concat([paired_df.copy(deep=True), paired_negative_epitope_df], axis=0)\n",
    "paired_with_negative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_ignore_for_duplicates = paired_with_negative_df.columns.difference([\"TCR_name\", \"Binding\"])\n",
    "paired_with_negative_df.drop_duplicates(inplace=True, subset=columns_to_ignore_for_duplicates, keep=\"first\")\n",
    "paired_with_negative_df[\"TCR_name\"] = range(1, len(paired_with_negative_df)+1)\n",
    "paired_with_negative_df.reset_index(drop=True, inplace=True)\n",
    "paired_with_negative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_path = \"../../data/customDatasets/negative_samples/\"\n",
    "file_name = \"paired_concatenated_with_negative.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_with_negative_df.to_csv(to_path+file_name, sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BA24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
