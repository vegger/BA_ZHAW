{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if not 'precision' in locals():\n",
    "  precision = \"gene\" # allele or gene\n",
    "\n",
    "if not 'input_file' in locals():\n",
    "  input_file = f\"../../data/customDatasets/{precision}/paired_concatenated.tsv\"\n",
    "df = pd.read_csv(input_file, sep='\\t', low_memory=False)\n",
    "\n",
    "if not 'paired_output_folder' in locals():\n",
    "  paired_output_folder = f\"../../data/splitted_data/{precision}/paired\"\n",
    "\n",
    "if not 'validation_file_name' in locals():\n",
    "  validation_file_name = \"validation.tsv\"\n",
    "\n",
    "if not 'test_file_name' in locals():\n",
    "  test_file_name = \"test.tsv\"\n",
    "\n",
    "if not 'train_file_name' in locals():\n",
    "  train_file_name = \"train.tsv\"\n",
    "\n",
    "if not 'aimed_test_ratio' in locals():\n",
    "  aimed_test_ratio = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the data entries (without negative data) is analysed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcr_key = \"tcr_key\"\n",
    "\n",
    "df[tcr_key] = df['TRA_CDR3'].astype(str) + '_' + df['TRB_CDR3'].astype(str)\n",
    "\n",
    "\n",
    "distinct_tcrs = df.drop_duplicates(subset=[tcr_key], keep=\"first\", inplace=False)\n",
    "unique_epitopes = df.drop_duplicates(subset=[\"Epitope\"], keep=False, inplace=False)\n",
    "unique_tcrs = df.drop_duplicates(subset=[tcr_key], keep=False, inplace=False)\n",
    "\n",
    "\n",
    "print(f\"distinct tcr's: {len(distinct_tcrs)} from {len(df)}\")\n",
    "print(f\"unique tcr's: {len(unique_tcrs)} from {len(df)}\")\n",
    "print(f\"unique epitopes: {len(unique_epitopes[\"Epitope\"])} from {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a train and test set is created. The test set should consist only of TPP2 and TPP3 Tasks (TPP => TCRâ€“Peptide/Epitope Pairing).\n",
    "TPP2 means the epitope is seen in training but TCR is unseen.\n",
    "TPP3 means neither the TCR nor the epitope is seen in training ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df, unique_tcrs, how='left', indicator=True)\n",
    "df_train = df_train[df_train['_merge'] == 'left_only']\n",
    "df_train[\"task\"] = \"\"\n",
    "train_epitopes = set(df_train[\"Epitope\"])\n",
    "\n",
    "df_test = unique_tcrs.copy()\n",
    "df_test[\"task\"] = df_test[\"Epitope\"].apply(lambda x: 'TPP3' if x not in train_epitopes else 'TPP2')\n",
    "\n",
    "number_of_TPP3 = (df_test['task'] == 'TPP3').sum()\n",
    "number_of_TPP2 = (df_test['task'] == 'TPP2').sum()\n",
    "number_of_TPP1 = (df_test['task'] == 'TPP1').sum()\n",
    "test_ratio = len(df_test)/(len(df_test) + len(df_train))\n",
    "\n",
    "print(f\"train data has {len(df_train)} entries\")\n",
    "print(f\"test data has {len(df_test)} entries\")\n",
    "print(f\"test data has {number_of_TPP1} TPP1 tasks (unseen tcr & seen epitopes).\")\n",
    "print(f\"test data has {number_of_TPP2} TPP2 tasks (unseen tcr & seen epitopes).\")\n",
    "print(f\"test data has {number_of_TPP3} TPP3 tasks (unseen tcr & unseen epitope).\")\n",
    "print(f\"the train/test ratio is {(1-test_ratio)}/{test_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the test ratio is below 0.3, we fill up the test data with TPP1 tasks (seen tcr & seen epitope). If the ratio is higher than 0.3, an exception get's thrown to prevent working with unbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpp1_from_train_to_test(amount, df_train, df_test):\n",
    "  number_of_TPP1 = (df_test['task'] == 'TPP1').sum() \n",
    "  number_of_TPP2 = (df_test['task'] == 'TPP2').sum()\n",
    "  print(f\"{amount} entries from train will be moved to test (TPP1)\")\n",
    "  print(f\"df_train size before: {len(df_train.index)}\")\n",
    "  print(f\"number of tpp1 before: {number_of_TPP1}\")\n",
    "  print(f\"number of tpp2 before: {number_of_TPP2}\")\n",
    "\n",
    "  for i in range(amount):\n",
    "    # Find values that appear more than once in each column\n",
    "    non_unique_epitopes = df_train['Epitope'].value_counts()\n",
    "    non_unique_epitopes = non_unique_epitopes[non_unique_epitopes > 1].index.tolist()\n",
    "\n",
    "    non_unique_CDR3 = df_train[tcr_key].value_counts()\n",
    "    non_unique_CDR3 = non_unique_CDR3[non_unique_CDR3 > 1].index.tolist()\n",
    "\n",
    "    # Filter df_train to only include rows where the Epitope and CDR3 values are not unique\n",
    "    filtered_df = df_train[df_train['Epitope'].isin(non_unique_epitopes) & df_train[tcr_key].isin(non_unique_CDR3)]\n",
    "    \n",
    "    if not filtered_df.empty:\n",
    "      first_row_index = filtered_df.index[0]\n",
    "      df_train.loc[first_row_index, 'task'] = \"TPP1\"\n",
    "      # Append this row to df_test\n",
    "      df_test = pd.concat([df_test, pd.DataFrame([df_train.loc[first_row_index]])], ignore_index=True)\n",
    "      # Drop this row from df_train using its index\n",
    "      df_train = df_train.drop(first_row_index)\n",
    "    else:\n",
    "      raise Exception(\"The specific row does not exist in df_train.\")\n",
    "    \n",
    "  number_of_TPP1 = (df_test['task'] == 'TPP1').sum() \n",
    "  number_of_TPP2 = (df_test['task'] == 'TPP2').sum()\n",
    "  print(f\"df_train size after: {len(df_train.index)}\")\n",
    "  print(f\"number of tpp1 after: {number_of_TPP1}\")\n",
    "  print(f\"number of tpp2 after: {number_of_TPP2}\")\n",
    "  return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "if(test_ratio > aimed_test_ratio):\n",
    "  abundant_test_data_count = math.ceil((test_ratio - aimed_test_ratio) * (len(df_test) + len(df_train)))\n",
    "  print(f\"{abundant_test_data_count} entries will be shifted from test to train so the train/test ratio can be {1-aimed_test_ratio}/{aimed_test_ratio}\")\n",
    "  # Filter and select rows\n",
    "  filtered_rows = df_test[df_test[\"task\"] == \"TPP2\"]\n",
    "  if len(filtered_rows) < abundant_test_data_count:\n",
    "      raise ValueError(\"Not enough entries with 'task' == 'TPP2' to move.\")\n",
    "  rows_to_move = filtered_rows.head(abundant_test_data_count)\n",
    "  # Append to df_train\n",
    "  df_train = pd.concat([df_train, rows_to_move], ignore_index=True)\n",
    "  # Remove from df_test\n",
    "  df_test = df_test.drop(rows_to_move.index)\n",
    "elif(test_ratio < aimed_test_ratio):\n",
    "  missing_test_data_count = math.ceil((aimed_test_ratio - test_ratio) * (len(df_test) + len(df_train)))\n",
    "  print(f\"{missing_test_data_count} entries need to be shifted from train to test so the train/test ratio can be {1-aimed_test_ratio}/{aimed_test_ratio}\")\n",
    "  df_train, df_test = tpp1_from_train_to_test(missing_test_data_count, df_train, df_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test data should have the same amount of TPP1 and TPP 2 Tasks. There is not enough data to have a proper TPP3 share so we just take as many as we can, without removing information from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recalculate TPP counts\n",
    "number_of_TPP3 = (df_test['task'] == 'TPP3').sum()\n",
    "number_of_TPP2 = (df_test['task'] == 'TPP2').sum()\n",
    "number_of_TPP1 = (df_test['task'] == 'TPP1').sum()\n",
    "tpp1_ratio = number_of_TPP1/(number_of_TPP1 + number_of_TPP2)\n",
    "\n",
    "aimed_tpp1_ratio = 0.5\n",
    "missing_tpp1_count = math.ceil((aimed_tpp1_ratio - tpp1_ratio) * (number_of_TPP2 + number_of_TPP1))\n",
    "abundant_tpp2_count = number_of_TPP2 - math.ceil((1-aimed_tpp1_ratio) * (number_of_TPP1 + number_of_TPP2))\n",
    "\n",
    "# move surplus TPP2 entries back to train\n",
    "print(f\"{abundant_tpp2_count} entries will be shifted from test to train so the tpp1/tpp2 ratio can be {aimed_tpp1_ratio}/{1-aimed_tpp1_ratio}\")\n",
    "# Filter and select rows\n",
    "filtered_rows = df_test[df_test[\"task\"] == \"TPP2\"]\n",
    "if len(filtered_rows) < abundant_tpp2_count:\n",
    "    raise ValueError(\"Not enough entries with 'task' == 'TPP2' to move.\")\n",
    "rows_to_move = filtered_rows.head(abundant_tpp2_count)\n",
    "# Append to df_train\n",
    "df_train = pd.concat([df_train, rows_to_move], ignore_index=True)\n",
    "# Remove from df_test\n",
    "df_test = df_test.drop(rows_to_move.index)\n",
    "\n",
    "# get TPP1 tasks from train and move them to test\n",
    "print(f\"{missing_tpp1_count} entries need to be shifted from train to test so the tpp1/tpp2 ratio can be {aimed_tpp1_ratio}/{1-aimed_tpp1_ratio}\")\n",
    "df_train, df_test = tpp1_from_train_to_test(missing_tpp1_count, df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_TPP1 = (df_test['task'] == 'TPP1').sum()\n",
    "number_of_TPP2 = (df_test['task'] == 'TPP2').sum()\n",
    "number_of_TPP3 = (df_test['task'] == 'TPP3').sum()\n",
    "test_ratio = len(df_test)/(len(df_test) + len(df_train))\n",
    "\n",
    "print(f\"train data has {len(df_train)} entries\")\n",
    "print(f\"test data has {len(df_test)} entries\")\n",
    "print(f\"test data has {number_of_TPP1} TPP1 tasks (seen tcr & seen epitopes).\")\n",
    "print(f\"test data has {number_of_TPP2} TPP2 tasks (unseen tcr & seen epitopes).\")\n",
    "print(f\"test data has {number_of_TPP3} TPP3 tasks (unseen tcr & unseen epitope).\")\n",
    "print(f\"the train/test ratio is {(1-test_ratio)}/{test_ratio}\")\n",
    "\n",
    "df_test.drop(columns=[\"_merge\", \"tcr_key\"], inplace=True, errors='ignore')\n",
    "df_train.drop(columns=[\"_merge\", \"tcr_key\"], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to verify each row\n",
    "def calculate_task(row, known_epitopes, known_tcr):\n",
    "    epitope_exists = row['Epitope'] in known_epitopes\n",
    "    row[tcr_key] = row['TRA_CDR3'] + '_' + row['TRB_CDR3']\n",
    "    cdr3_exists = row[tcr_key] in known_tcr\n",
    "    \n",
    "    if epitope_exists and cdr3_exists:\n",
    "        return 'TPP1'\n",
    "    elif epitope_exists and not cdr3_exists:\n",
    "        return 'TPP2'\n",
    "    elif not epitope_exists and not cdr3_exists:\n",
    "        return 'TPP3'\n",
    "    raise Exception(\"Something seems wrong\")  # This handles unexpected cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "tpp1_df = df_test[df_test['task'] == 'TPP1']\n",
    "tpp2_df = df_test[df_test['task'] == 'TPP2']\n",
    "tpp3_df = df_test[df_test['task'] == 'TPP3']\n",
    "\n",
    "# Shuffle and split tpp1 entries\n",
    "tpp1_df = shuffle(tpp1_df, random_state=42)\n",
    "split_index = len(tpp1_df) // 2\n",
    "val_tpp1 = tpp1_df[:split_index]\n",
    "test_tpp1 = tpp1_df[split_index:]\n",
    "\n",
    "# Shuffle and split tpp2 entries\n",
    "tpp2_df = shuffle(tpp2_df, random_state=42)\n",
    "split_index = len(tpp2_df) // 2\n",
    "val_tpp2 = tpp2_df[:split_index]\n",
    "test_tpp2 = tpp2_df[split_index:]\n",
    "\n",
    "# Shuffle and split tpp3 entries\n",
    "tpp3_df = shuffle(tpp3_df, random_state=42)\n",
    "split_index = len(tpp3_df) // 2\n",
    "val_tpp3 = tpp3_df[:split_index]\n",
    "test_tpp3 = tpp3_df[split_index:]\n",
    "\n",
    "#df_test = pd.concat([test_tpp2, test_tpp3])\n",
    "df_test = pd.concat([test_tpp1, test_tpp2, test_tpp3])\n",
    "df_validation = pd.concat([val_tpp1, val_tpp2, val_tpp3])\n",
    "\n",
    "# Recalculate TPP3 classification in test\n",
    "df_train_val = pd.concat([df_train, df_validation])\n",
    "df_train_val[tcr_key] = df_train_val['TRA_CDR3'].astype(str) + '_' + df_train_val['TRB_CDR3'].astype(str)\n",
    "\n",
    "seen_epitopes = set(df_train_val[\"Epitope\"])\n",
    "seen_tcr = set(df_train_val[tcr_key])\n",
    "df_test[\"task\"] = df_test.apply(lambda x: calculate_task(x, seen_epitopes, seen_tcr), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_TPP1 = (df_test['task'] == 'TPP1').sum()\n",
    "number_of_TPP2 = (df_test['task'] == 'TPP2').sum()\n",
    "number_of_TPP3 = (df_test['task'] == 'TPP3').sum()\n",
    "test_ratio = len(df_test)/(len(df_test) + len(df_train) + len(df_validation))\n",
    "validation_ratio = len(df_validation)/(len(df_test) + len(df_train) + len(df_validation))\n",
    "\n",
    "print(f\"test data has {len(df_test)} entries\")\n",
    "print(f\"validation data has {len(df_validation)} entries\")\n",
    "print(f\"train data has {len(df_train)} entries\")\n",
    "print(f\"test data has {number_of_TPP1} TPP1 tasks (seen tcr & seen epitopes).\")\n",
    "print(f\"test data has {number_of_TPP2} TPP2 tasks (unseen tcr & seen epitopes).\")\n",
    "print(f\"test data has {number_of_TPP3} TPP3 tasks (unseen tcr & unseen epitope).\")\n",
    "print(f\"the test ratio is {(1-test_ratio)}/{test_ratio}\")\n",
    "print(f\"the validation ratio is {(1-validation_ratio)}/{validation_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(columns=[\"is_duplicated\"], inplace=True)\n",
    "df_train[\"task\"] = np.nan\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation.drop(columns=[\"is_duplicated\"], inplace=True)\n",
    "df_validation[\"task\"] = np.nan\n",
    "df_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop(columns=[\"is_duplicated\"], inplace=True)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation.to_csv(f\"{paired_output_folder}/{validation_file_name}\", sep=\"\\t\", index=False)\n",
    "df_test.to_csv(f\"{paired_output_folder}/{test_file_name}\", sep=\"\\t\", index=False)\n",
    "df_train.to_csv(f\"{paired_output_folder}/{train_file_name}\", sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BA24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
