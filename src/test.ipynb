{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./samples.tsv\", sep=\"\\t\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df = df.drop([\"Epitope Embedding\", \"TRA_CDR3 Embedding\", \"TRB_CDR3 Embedding\"], axis=1)\n",
    "# df = df.drop([\"Unnamed: 0\"], axis=1)\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"./samples.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "epitope_embeddings_path = \"./paired/paired_epitope_embeddings.npz\"\n",
    "tra_embeddings_path = \"./paired/paired_TRA_CDR3_embeddings.npz\"\n",
    "trb_embeddings_path = \"./paired/paired_TRB_CDR3_embeddings.npz\"\n",
    "\n",
    "# Load the entire DataFrame\n",
    "data_frame = df\n",
    "\n",
    "# Memory-map the embeddings\n",
    "epitope_embeddings = np.load(epitope_embeddings_path)\n",
    "tra_embeddings = np.load(tra_embeddings_path)\n",
    "trb_embeddings = np.load(trb_embeddings_path)\n",
    "\n",
    "data_frame[\"Epitope Embedding\"] = data_frame[\"Epitope\"].map(epitope_embeddings)\n",
    "data_frame[\"TRA_CDR3 Embedding\"] = data_frame[\"TRA_CDR3\"].map(tra_embeddings)\n",
    "data_frame[\"TRB_CDR3 Embedding\"] = data_frame[\"TRB_CDR3\"].map(trb_embeddings)\n",
    "\n",
    "columns = list(data_frame.columns)  # Get the list of all column names\n",
    "columns.remove('Binding')\n",
    "columns.append('Binding')\n",
    "data_frame = data_frame[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"Epitope Embedding\", \"TRA_CDR3 Embedding\", \"TRB_CDR3 Embedding\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epitope_embeddings = []\n",
    "tra_cdr3_embeddings = []\n",
    "trb_cdr3_embeddings = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    epitope_embeddings.append(row[\"Epitope Embedding\"])\n",
    "    tra_cdr3_embeddings.append(row[\"TRA_CDR3 Embedding\"])\n",
    "    trb_cdr3_embeddings.append(row[\"TRB_CDR3 Embedding\"])\n",
    "\n",
    "max_length = max(\n",
    "    max(embedding.shape[0] for embedding in epitope_embeddings),\n",
    "    max(embedding.shape[0] for embedding in tra_cdr3_embeddings),\n",
    "    max(embedding.shape[0] for embedding in trb_cdr3_embeddings)\n",
    ")\n",
    "\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct iteration over a Series\n",
    "for index, value in df['Epitope Embedding'].items():\n",
    "    print(f\"Index: {index}, Value: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct iteration over a Series\n",
    "for index, value in df['TRA_CDR3 Embedding'].items():\n",
    "    print(f\"Index: {index}, Value: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct iteration over a Series\n",
    "for index, value in df['TRB_CDR3 Embedding'].items():\n",
    "    print(f\"Index: {index}, Value: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# function used for the padding of the embeddings\n",
    "def pad_collate(batch):\n",
    "    # Initialize lists to hold the embeddings and labels\n",
    "    epitope_embeddings, tra_cdr3_embeddings, trb_cdr3_embeddings, labels = [], [], [], []\n",
    "\n",
    "    # Extract embeddings and labels from the batch\n",
    "    for item in batch:\n",
    "        epitope_embeddings.append(item[\"epitope_embedding\"])\n",
    "        tra_cdr3_embeddings.append(item[\"tra_cdr3_embedding\"])\n",
    "        trb_cdr3_embeddings.append(item[\"trb_cdr3_embedding\"])\n",
    "        labels.append(item[\"label\"])\n",
    "\n",
    "\n",
    "    # TODO instead of max the batch we need to max to the entire dataset \n",
    "    # otherwise we find a way to write this max_length into this train.py. \n",
    "    # However if i overwrite IN_CHANNELS in this function here does not work because gets executed in Dataloader...\n",
    "    # Find the maximum length across all embeddings in the batch\n",
    "    max_length = max(\n",
    "        max(embedding.size(0) for embedding in epitope_embeddings),\n",
    "        max(embedding.size(0) for embedding in tra_cdr3_embeddings),\n",
    "        max(embedding.size(0) for embedding in trb_cdr3_embeddings)\n",
    "    )\n",
    "\n",
    "    # Function to pad embeddings to the maximum length\n",
    "    def pad_embeddings(embeddings):\n",
    "        return torch.stack([\n",
    "            torch.nn.functional.pad(embedding, (0, 0, 0, max_length - embedding.size(0)), \"constant\", 0)\n",
    "            for embedding in embeddings\n",
    "        ])\n",
    "\n",
    "    # Pad and stack all embeddings and labels\n",
    "    epitope_embeddings = pad_embeddings(epitope_embeddings)\n",
    "    tra_cdr3_embeddings = pad_embeddings(tra_cdr3_embeddings)\n",
    "    trb_cdr3_embeddings = pad_embeddings(trb_cdr3_embeddings)\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    print(f\"this is in pad_collate: epitope_embedding:\\n{epitope_embeddings.shape}\")\n",
    "    return {\n",
    "        \"epitope_embedding\": epitope_embeddings,\n",
    "        \"tra_cdr3_embedding\": tra_cdr3_embeddings,\n",
    "        \"trb_cdr3_embedding\": trb_cdr3_embeddings,\n",
    "        \"label\": labels\n",
    "    } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(df[\"Epitope Embedding\"][0], dtype=torch.float32).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_epitope_embeddings = []\n",
    "\n",
    "list_epitope_embeddings.append(torch.tensor(df[\"Epitope Embedding\"][0], dtype=torch.float32))\n",
    "\n",
    "list_epitope_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_epitope_embeddings[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 20\n",
    "def pad_embeddings(embeddings):\n",
    "        return torch.stack([\n",
    "            torch.nn.functional.pad(embedding, (0, 0, 0, max_length - embedding.shape[0]), \"constant\", 0)\n",
    "            for embedding in embeddings\n",
    "        ])\n",
    "\n",
    "stack = pad_embeddings(list_epitope_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BA24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
