{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set precision of mhc and V/J values (gene or allele)\n",
    "precision = 'allele'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is not thread safe\n",
    "def create_folders_if_not_exists(folders):\n",
    "  for path in folders:\n",
    "    if not os.path.exists(path):\n",
    "      os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_data = './data'\n",
    "pipeline_data_plain = f'{pipeline_data}/plain_datasets'\n",
    "pipeline_data_cleaned = f'{pipeline_data}/cleaned_datasets'\n",
    "pipeline_data_concatenated = f'{pipeline_data}/concatenated_datasets'\n",
    "pipeline_data_splitted = f'{pipeline_data}/splitted_datasets'\n",
    "pipeline_data_temp_bucket = f'{pipeline_data}/temp'\n",
    "\n",
    "pipeline_folders = [pipeline_data, pipeline_data_plain, pipeline_data_cleaned, pipeline_data_concatenated, pipeline_data_splitted, pipeline_data_temp_bucket]\n",
    "\n",
    "create_folders_if_not_exists(pipeline_folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IEDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare directories\n",
    "IEDB_data_plain = f'{pipeline_data_plain}/IEDB'\n",
    "IEDB_data_cleaned = f'{pipeline_data_cleaned}/IEDB'\n",
    "IEDB_data_fitted = f'{pipeline_data_temp_bucket}/IEDB'\n",
    "\n",
    "IEDB_folders = [IEDB_data_plain, IEDB_data_cleaned, IEDB_data_fitted]\n",
    "create_folders_if_not_exists(IEDB_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook IEDB fit data\n",
    "path_prefix_plain = IEDB_data_plain\n",
    "path_prefix_fitted = IEDB_data_fitted\n",
    "mhc_I_input_beta = f\"{path_prefix_plain}/MHCI_IEDB_beta_export.csv\"\n",
    "mhc_I_output_beta = f\"{path_prefix_fitted}/IEDB_beta_fitted.csv\"\n",
    "mhc_I_input_paired = f\"{path_prefix_plain}/MHCI_IEDB_paired_export.csv\"\n",
    "mhc_I_output_paired = f\"{path_prefix_fitted}/IEDB_paired_fitted.csv\"\n",
    "\n",
    "# fit IEDB data\n",
    "%run ./data_scripts/IEDB/IEDB_fitted_dataset.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook IEDB clean data\n",
    "path_prefix_fitted = IEDB_data_fitted\n",
    "path_prefix_cleaned =  IEDB_data_cleaned\n",
    "fitted_file_beta = \"IEDB_beta_fitted.csv\"\n",
    "fitted_file_paired = \"IEDB_paired_fitted.csv\"\n",
    "cleaned_file_beta = \"IEDB_cleaned_data_beta.csv\"\n",
    "cleaned_file_paired = \"IEDB_cleaned_data_paired.csv\"\n",
    "\n",
    "# clean IEDB data\n",
    "%run ./data_scripts/IEDB/IEDB_clean_dataset.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IEDB_cleaned_beta_output = f'{IEDB_data_cleaned}/{cleaned_file_beta}'\n",
    "IEDB_cleaned_paired_output = f'{IEDB_data_cleaned}/{cleaned_file_paired}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### McPAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare directories\n",
    "McPas_data_plain = f'{pipeline_data_plain}/McPas'\n",
    "McPas_data_cleaned = f'{pipeline_data_cleaned}/McPas'\n",
    "McPas_data_fitted = f'{pipeline_data_temp_bucket}/McPas'\n",
    "\n",
    "McPas_folders = [McPas_data_plain, McPas_data_cleaned, McPas_data_fitted]\n",
    "create_folders_if_not_exists(McPas_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook McPAS fit data\n",
    "input_file = f'{McPas_data_plain}/McPAS-TCR.csv'\n",
    "path_prefix_fitted = McPas_data_fitted\n",
    "fitted_file = 'McPAS_fitted.tsv'\n",
    "\n",
    "# fit McPAS data\n",
    "%run ./data_scripts/McPas-TCR/fit_data_mcpastcr_both.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook McPAS clean data\n",
    "fitted_input_file = f'{McPas_data_fitted}/{fitted_file}'\n",
    "path_prefix_cleaned = McPas_data_cleaned\n",
    "cleaned_file_paired = 'McPAS_cleaned_data_paired.tsv'\n",
    "cleaned_file_beta = 'McPAS_cleaned_data_beta.tsv'\n",
    "\n",
    "# clean McPAS data\n",
    "%run ./data_scripts/McPas-TCR/clean_data_mcpastcr_both.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "McPAS_cleaned_beta_output = f'{McPas_data_cleaned}/{cleaned_file_beta}'\n",
    "McPAS_cleaned_paired_output = f'{McPas_data_cleaned}/{cleaned_file_paired}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VDJdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare directories\n",
    "VDJdb_data_plain = f'{pipeline_data_plain}/VDJdb'\n",
    "VDJdb_data_cleaned = f'{pipeline_data_cleaned}/VDJdb'\n",
    "VDJdb_data_fitted = f'{pipeline_data_temp_bucket}/VDJdb'\n",
    "\n",
    "VDJdb_folders = [VDJdb_data_plain, VDJdb_data_cleaned, VDJdb_data_fitted]\n",
    "create_folders_if_not_exists(VDJdb_folders)\n",
    "\n",
    "fitted_beta_file = 'VDJdb_beta_fitted.tsv'\n",
    "fitted_paired_file = 'VDJdb_paired_fitted.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook VDJdb fit data paired\n",
    "input_file = f'{VDJdb_data_plain}/VDJdb_paired_only.tsv'\n",
    "path_prefix_fitted = VDJdb_data_fitted\n",
    "fitted_file = fitted_paired_file\n",
    "\n",
    "# fit paired VDJdb data\n",
    "%run ./data_scripts/VDJdb/fit_data_vdjdb_paired.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook VDJdb fit data beta\n",
    "input_file = f'{VDJdb_data_plain}/VDJdb_beta_only.tsv'\n",
    "path_prefix_fitted = VDJdb_data_fitted\n",
    "fitted_file = fitted_beta_file\n",
    "\n",
    "# fit beta VDJdb data\n",
    "%run ./data_scripts/VDJdb/fit_data_vdjdb_beta.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook VDJdb clean data paired\n",
    "input_file = f'{VDJdb_data_fitted}/{fitted_paired_file}'\n",
    "cleaned_file_paired = 'VDJdb_cleaned_data_paired.tsv'\n",
    "output_file = f'{VDJdb_data_cleaned}/{cleaned_file_paired}'\n",
    "\n",
    "# clean paired VDJdb data\n",
    "%run ./data_scripts/VDJdb/clean_data_vdjdb_paired.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook VDJdb clean data beta\n",
    "input_file = f'{VDJdb_data_fitted}/{fitted_beta_file}'\n",
    "cleaned_file_beta = 'VDJdb_cleaned_data_beta.tsv'\n",
    "output_file = f'{VDJdb_data_cleaned}/{cleaned_file_beta}'\n",
    "\n",
    "# clean beta VDJdb data\n",
    "%run ./data_scripts/VDJdb/clean_data_vdjdb_beta.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VDJdb_cleaned_beta_output = f'{VDJdb_data_cleaned}/{cleaned_file_beta}'\n",
    "VDJdb_cleaned_paired_output = f'{VDJdb_data_cleaned}/{cleaned_file_paired}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Concatenation\n",
    "The concatenation includes further cleaning and advanced removal of duplicated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for concatenation\n",
    "custom_dataset_path = f'{pipeline_data_concatenated}/{precision}/'\n",
    "\n",
    "# beta input files\n",
    "vdjdb_beta_read_path = VDJdb_cleaned_beta_output\n",
    "mcpastcr_beta_read_path = McPAS_cleaned_beta_output\n",
    "iedb_beta_read_path = IEDB_cleaned_beta_output\n",
    "# paired input files\n",
    "vdjdb_paired_read_path = VDJdb_cleaned_paired_output\n",
    "mcpastcr_paired_read_path = McPAS_cleaned_paired_output\n",
    "iedb_paired_read_path = IEDB_cleaned_paired_output\n",
    "# output files\n",
    "output_file_beta = 'beta_concatenated.tsv'\n",
    "output_file_paired = 'paired_concatenated.tsv'\n",
    "\n",
    "create_folders_if_not_exists([custom_dataset_path])\n",
    "\n",
    "%run ./data_scripts/concatDatasets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_paired = f'{custom_dataset_path}/{output_file_paired}'\n",
    "concatenated_beta = f'{custom_dataset_path}/{output_file_beta}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split\n",
    "The split creates 3 datasets. Train, Validation and Test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for split of paired dataset\n",
    "input_file = concatenated_paired\n",
    "paired_output_folder = f'{pipeline_data_splitted}/{precision}/paired'\n",
    "validation_file_name = 'validation.tsv'\n",
    "test_file_name = 'test.tsv'\n",
    "train_file_name = 'train.tsv'\n",
    "aimed_test_ratio = 0.3 # this means 30% of the concatenated dataset will be for test and validation (fifty/fifty)\n",
    "\n",
    "create_folders_if_not_exists([paired_output_folder])\n",
    "\n",
    "# do the split\n",
    "%run ./data_scripts/data_preparation/split_paired.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for split of beta dataset\n",
    "input_file = concatenated_beta\n",
    "beta_output_folder = f'{pipeline_data_splitted}/{precision}/beta'\n",
    "aimed_test_ratio = 0.3 # this means 30% of the concatenated dataset will be for test and validation (fifty/fifty)\n",
    "\n",
    "create_folders_if_not_exists([beta_output_folder])\n",
    "\n",
    "# do the split\n",
    "%run ./data_scripts/data_preparation/split_beta.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for paired dataset\n",
    "read_path_train = f'{paired_output_folder}/{train_file_name}'\n",
    "read_path_test = f'{paired_output_folder}/{test_file_name}'\n",
    "read_path_validation = f'{paired_output_folder}/{validation_file_name}'\n",
    "temp_path = f'{pipeline_data_temp_bucket}/negative_samples/paired/'\n",
    "output_path = paired_output_folder  # we are not interested in the positive only data so we override them with positive/negative dataset\n",
    "train_output_name = train_file_name\n",
    "validation_output_name = validation_file_name\n",
    "test_output_name = test_file_name\n",
    "\n",
    "create_folders_if_not_exists([temp_path])\n",
    "\n",
    "%run ./data_scripts/negative_samples/negative_samples_paired.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for beta dataset\n",
    "read_path_train = f'{beta_output_folder}/{train_file_name}'\n",
    "read_path_test = f'{beta_output_folder}/{test_file_name}'\n",
    "read_path_validation = f'{beta_output_folder}/{validation_file_name}'\n",
    "temp_path = f'{pipeline_data_temp_bucket}/negative_samples/beta/'\n",
    "output_path = beta_output_folder  # we are not interested in the positive only data so we override them with positive/negative dataset\n",
    "train_output_name = train_file_name\n",
    "validation_output_name = validation_file_name\n",
    "test_output_name = test_file_name\n",
    "\n",
    "create_folders_if_not_exists([temp_path])\n",
    "\n",
    "%run ./data_scripts/negative_samples/negative_samples_beta.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Classification\n",
    "The classification in the split notebook correct for positive only data. After adding negative data, some classifications might be wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the classification for paired data\n",
    "paired = True\n",
    "train_data_path = f'{paired_output_folder}/{train_file_name}'\n",
    "test_data_path = f'{paired_output_folder}/{test_file_name}'\n",
    "validation_data_path = f'{paired_output_folder}/{validation_file_name}'\n",
    "\n",
    "%run ./data_scripts/data_preparation/classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extended classification for paired data\n",
    "test_path = f'{paired_output_folder}/{test_file_name}'\n",
    "train_path = f'{paired_output_folder}/{train_file_name}'\n",
    "validation_path = f'{paired_output_folder}/{validation_file_name}'\n",
    "output_path = f'{paired_output_folder}/test_reclassified_paired_specific.tsv'\n",
    "paired_data_path = paired_output_folder\n",
    "alpha_cdr3_name = 'TRA_CDR3'\n",
    "beta_cdr3_name = 'TRB_CDR3'\n",
    "epitope_name = 'Epitope'\n",
    "task_name = 'task'\n",
    "\n",
    "%run ./data_scripts/data_preparation/paired_reclassification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the classification for beta data\n",
    "paired = False\n",
    "train_data_path = f'{beta_output_folder}/{train_file_name}'\n",
    "test_data_path = f'{beta_output_folder}/{test_file_name}'\n",
    "validation_data_path = f'{beta_output_folder}/{validation_file_name}'\n",
    "\n",
    "%run ./data_scripts/data_preparation/classification.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next two cells the classification is checked. If the output says \"Classification is correct\", everything is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check task classification paired\n",
    "splitted_data_path = paired_output_folder\n",
    "\n",
    "%run ./data_scripts/data_preparation/check_task_classification_paired.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check task classification beta\n",
    "splitted_data_path = beta_output_folder\n",
    "\n",
    "%run ./data_scripts/data_preparation/check_task_classification_beta.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise Exception(\"Prevent upload\")\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# load_dotenv()\n",
    "\n",
    "# # upload paired data\n",
    "# path_to_data = f'{pipeline_data_splitted}/{precision}/paired'\n",
    "# dataset_name = f'paired_{precision}'\n",
    "# main_project_name = os.getenv(\"MAIN_PROJECT_NAME\")\n",
    "\n",
    "# %run ./data_scripts/upload_datasets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # upload beta data\n",
    "# path_to_data = f'{pipeline_data_splitted}/{precision}/beta'\n",
    "# dataset_name = f'beta_{precision}'\n",
    "\n",
    "# %run ./data_scripts/upload_datasets.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_paired_test = f\"data/splitted_datasets/{precision}/paired/test.tsv\"\n",
    "path_paired_validation = f\"data/splitted_datasets/{precision}/paired/validation.tsv\"\n",
    "path_paired_train = f\"data/splitted_datasets/{precision}/paired/train.tsv\"\n",
    "path_beta_test = f\"data/splitted_datasets/{precision}/beta/test.tsv\"\n",
    "path_beta_validation = f\"data/splitted_datasets/{precision}/beta/validation.tsv\"\n",
    "path_beta_train = f\"data/splitted_datasets/{precision}/beta/train.tsv\"\n",
    "\n",
    "# paired test\n",
    "%run ./data_scripts/generateEmbeddings.py paired {path_paired_test} TRA_paired_embeddings.npz TRA_CDR3 embeddings_\n",
    "%run ./data_scripts/generateEmbeddings.py paired {path_paired_test} TRB_paired_embeddings.npz TRB_CDR3 embeddings_\n",
    "%run ./data_scripts/generateEmbeddings.py paired {path_paired_test} Epitope_paired_embeddings.npz Epitope embeddings_\n",
    "\n",
    "# paired validation\n",
    "%run ./data_scripts/generateEmbeddings.py paired {path_paired_validation} TRA_paired_embeddings.npz TRA_CDR3 embeddings_\n",
    "%run ./data_scripts/generateEmbeddings.py paired {path_paired_validation} TRB_paired_embeddings.npz TRB_CDR3 embeddings_\n",
    "%run ./data_scripts/generateEmbeddings.py paired {path_paired_validation} Epitope_paired_embeddings.npz Epitope embeddings_\n",
    "\n",
    "# paired train\n",
    "%run ./data_scripts/generateEmbeddings.py paired {path_paired_train} TRA_paired_embeddings.npz TRA_CDR3 embeddings_\n",
    "%run ./data_scripts/generateEmbeddings.py paired {path_paired_train} TRB_paired_embeddings.npz TRB_CDR3 embeddings_\n",
    "%run ./data_scripts/generateEmbeddings.py paired {path_paired_train} Epitope_paired_embeddings.npz Epitope embeddings_\n",
    "\n",
    "# beta test\n",
    "%run beta ./data_scripts/generateEmbeddings.py {path_beta_test} TRB_beta_embeddings.npz TRB_CDR3 embeddings_\n",
    "%run beta ./data_scripts/generateEmbeddings.py {path_beta_test} Epitope_beta_embeddings.npz Epitope embeddings_\n",
    "\n",
    "# beta validation\n",
    "%run beta ./data_scripts/generateEmbeddings.py {path_beta_validation} TRB_beta_embeddings.npz TRB_CDR3 embeddings_\n",
    "%run beta ./data_scripts/generateEmbeddings.py {path_beta_validation} Epitope_beta_embeddings.npz Epitope embeddings_\n",
    "\n",
    "# beta train\n",
    "%run beta ./data_scripts/generateEmbeddings.py {path_beta_train} TRB_beta_embeddings.npz TRB_CDR3 embeddings_\n",
    "%run beta ./data_scripts/generateEmbeddings.py {path_beta_train} Epitope_beta_embeddings.npz Epitope embeddings_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Physicochemical Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./data_scripts/generatePhysicoParallel.py paired {pipeline_data_splitted}/{precision}/paired test ./data/physicoProperties {precision}\n",
    "!python ./data_scripts/generatePhysicoParallel.py paired {pipeline_data_splitted}/{precision}/paired validation ./data/physicoProperties {precision}\n",
    "!python ./data_scripts/generatePhysicoParallel.py paired {pipeline_data_splitted}/{precision}/paired train ./data/physicoProperties {precision}\n",
    "\n",
    "!python ./data_scripts/generatePhysicoParallel.py beta {pipeline_data_splitted}/{precision}/beta test ./data/physicoProperties {precision}\n",
    "!python ./data_scripts/generatePhysicoParallel.py beta {pipeline_data_splitted}/{precision}/beta validation ./data/physicoProperties {precision}\n",
    "!python ./data_scripts/generatePhysicoParallel.py beta {pipeline_data_splitted}/{precision}/beta train ./data/physicoProperties {precision}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Physicochemical Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"./data/physicoProperties\"\n",
    "chain = \"paired\"\n",
    "#%run ./data_scripts/scale_physicos.ipynb\n",
    "\n",
    "chain = \"beta\"\n",
    "%run ./data_scripts/scale_physicos.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BA24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
