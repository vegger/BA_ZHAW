{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tidytcells as tt\n",
    "\n",
    "if not 'precision' in locals():\n",
    "  precision = \"gene\" # possible values are gene and allele\n",
    "\n",
    "if not 'custom_dataset_path' in locals():\n",
    "  custom_dataset_path = \"../data/customDatasets/\" + precision + \"/\"\n",
    "\n",
    "if not 'vdjdb_beta_read_path' in locals():\n",
    "  vdjdb_beta_read_path = \"../data/VDJdb/cleaned_data/vdjdb_cleaned_data_beta.tsv\"\n",
    "vdjdb_beta_df = pd.read_csv(vdjdb_beta_read_path, sep=\"\\t\")\n",
    "\n",
    "if not 'mcpastcr_beta_read_path' in locals():\n",
    "  mcpastcr_beta_read_path = \"../data/McPAS-TCR/cleaned_data/mcpastcr_cleaned_data_beta.tsv\"\n",
    "mcpastcr_beta_df = pd.read_csv(mcpastcr_beta_read_path, sep=\"\\t\")\n",
    "\n",
    "if not 'iedb_beta_read_path' in locals():\n",
    "  iedb_beta_read_path = \"../data/IEDB/cleaned_data/IEDB_cleaned_data_beta.csv\"\n",
    "iedb_beta_df = pd.read_csv(iedb_beta_read_path)\n",
    "\n",
    "if not 'vdjdb_paired_read_path' in locals():\n",
    "  vdjdb_paired_read_path = \"../data/VDJdb/cleaned_data/vdjdb_cleaned_data_paired.tsv\"\n",
    "vdjdb_paired_df = pd.read_csv(vdjdb_paired_read_path, sep=\"\\t\")\n",
    "\n",
    "if not 'mcpastcr_paired_read_path' in locals():\n",
    "  mcpastcr_paired_read_path = \"../data/McPAS-TCR/cleaned_data/mcpastcr_cleaned_data_paired.tsv\"\n",
    "mcpastcr_paired_df = pd.read_csv(mcpastcr_paired_read_path, sep=\"\\t\")\n",
    "\n",
    "if not 'iedb_paired_read_path' in locals():\n",
    "  iedb_paired_read_path = \"../data/IEDB/cleaned_data/IEDB_cleaned_data_paired.csv\"\n",
    "iedb_paired_df = pd.read_csv(iedb_paired_read_path)\n",
    "\n",
    "if not 'output_file_beta' in locals():\n",
    "  output_file_beta = \"beta_concatenated.tsv\"\n",
    "\n",
    "if not 'output_file_paired' in locals():\n",
    "  output_file_paired = \"paired_concatenated.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_mhc(val):\n",
    "  if isinstance(val, str): return tt.mh.standardize(val, species=\"homosapiens\", precision=precision)\n",
    "  else: return np.nan\n",
    "\n",
    "def standardize_vj(val):\n",
    "  if isinstance(val, str): return tt.tr.standardize(gene=val, species=\"homosapiens\", precision=precision)\n",
    "  else: return np.nan\n",
    "\n",
    "def standardize_cdr3(val):\n",
    "  if isinstance(val, str): return tt.junction.standardize(seq=val)\n",
    "  else: return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_df = pd.concat([vdjdb_beta_df, mcpastcr_beta_df, iedb_beta_df], axis=0)\n",
    "obsolete_columns = [\"TRA_leader\", \"TRB_leader\", \"Linker\", \"Link_order\", \"TRA_5_prime_seq\", \"TRA_3_prime_seq\", \"TRB_5_prime_seq\", \"TRB_3_prime_seq\",\\\n",
    "                    \"Score\", \"MHC class\", \"TRAC\", \"TRAV\" ,\"TRAJ\", \"TRA_CDR3\"]\n",
    "\n",
    "beta_df = beta_df.drop(columns=obsolete_columns)\n",
    "print(f\"length of beta_df: {len(beta_df.index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before dropping duplicates we need to order the entries based on how many informations they contain\n",
    "beta_df[\"info_score\"] = beta_df.notnull().sum(axis=1)\n",
    "beta_df = beta_df.sort_values(by=['info_score'], ascending=[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_df[\"MHC\"] = beta_df[\"MHC\"].apply(standardize_mhc)\n",
    "beta_df[\"TRBV\"] = beta_df[\"TRBV\"].apply(standardize_vj)\n",
    "beta_df[\"TRBJ\"] = beta_df[\"TRBJ\"].apply(standardize_vj)\n",
    "beta_df[\"TRB_CDR3\"] = beta_df[\"TRB_CDR3\"].apply(standardize_cdr3)\n",
    "beta_df = beta_df.dropna(subset=[\"TRB_CDR3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The following script removes a lot of rows. They are kept and some of them get added again later\")\n",
    "cultivated_columns = beta_df.columns.difference([\"TCR_name\", \"info_score\"]).tolist()\n",
    "most_important_columns = [\"TRB_CDR3\", \"Epitope\"]\n",
    "\n",
    "beta_df_distinct = beta_df.drop_duplicates(subset=cultivated_columns, keep=\"first\")\n",
    "print(f\"distinct entries (all columns, keep=first). {len(beta_df.index)-len(beta_df_distinct.index)} entries removed.\")\n",
    "\n",
    "beta_df_no_duplicates = beta_df_distinct.drop_duplicates(subset=most_important_columns, keep=False)\n",
    "print(f\"removed all duplicates (CDR3, Epitope) from distinct values (most_important_columns, keep=False). {len(beta_df_distinct.index)-len(beta_df_no_duplicates.index)} entries removed.\")\n",
    "\n",
    "beta_df_removed_entries = pd.merge(beta_df_distinct, beta_df_no_duplicates, how=\"left\", indicator=True)\n",
    "beta_df_removed_entries = beta_df_removed_entries[beta_df_removed_entries['_merge'] == 'left_only'] # left-only values from left-join merge are dropped out rows\n",
    "print(f\"beta removed entries df length: {len(beta_df_removed_entries.index)}\")\n",
    "print()\n",
    "print()\n",
    "\n",
    "# Check data integrity and make sure beta_df_removed_entries and beta_df_no_duplicates have no same rows\n",
    "beta_df_removed_entries.drop(\"_merge\", axis='columns', inplace=True)\n",
    "merged_df = pd.merge(beta_df_removed_entries, beta_df_no_duplicates, on=most_important_columns, indicator=True, how='inner')\n",
    "# Check if merged_df is empty\n",
    "if not merged_df.empty:\n",
    "  print(merged_df)  # Optional: Display the common rows\n",
    "  print(\"There are identical rows between the two DataFrames.\")\n",
    "  raise Exception(\"ERROR: There shouldn't be identical rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_duplicate(base_row, compare_row, columns):\n",
    "    if compare_row['is_duplicated'] is True: return True\n",
    "\n",
    "    # print()\n",
    "    # print()\n",
    "    # print(\"base row:\")\n",
    "    # print(base_row)\n",
    "    # print()\n",
    "    # print(\"comparing row:\")\n",
    "    # print(compare_row)\n",
    "\n",
    "    for key in columns:\n",
    "      if base_row[key] != compare_row[key] and not pd.isna(compare_row[key]):\n",
    "        return False\n",
    "    \n",
    "\n",
    "    # print(\"*********************DUPLICATION*******************************\")\n",
    "    # print(\"base row:\")\n",
    "    # print(base_row)\n",
    "    # print()\n",
    "    # print(\"comparing row:\")\n",
    "    # print(compare_row)\n",
    "\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare both lists\n",
    "beta_df_removed_entries['is_duplicated'] = False\n",
    "duplicates_to_add = pd.DataFrame(columns=beta_df_removed_entries.columns)\n",
    "\n",
    "# Iterate from top to bottom (top has highest information score)\n",
    "beta_df_removed_grouped = beta_df_removed_entries.groupby(most_important_columns)\n",
    "number_of_groups = len(beta_df_removed_grouped)\n",
    "print(\"Number of groups formed:\", number_of_groups)\n",
    "\n",
    "for name, group in beta_df_removed_grouped:\n",
    "  group = group.sort_values(by=['info_score'], ascending=False).reset_index(drop=True)\n",
    "  # print(f\"group {name}\")\n",
    "  # print(group)\n",
    "  # print()\n",
    "  # print()\n",
    "\n",
    "  for i in range(len(group.index)-1):\n",
    "    if group.iloc[i]['is_duplicated'].any(): continue\n",
    "\n",
    "    for j in range(i+1, len(group.index)):\n",
    "      if not group.iloc[j]['is_duplicated'].any() and is_duplicate(group.iloc[i], group.iloc[j], cultivated_columns):\n",
    "        group.at[j, 'is_duplicated'] = True\n",
    "    \n",
    "  duplicates_to_add = pd.concat([duplicates_to_add, group[group['is_duplicated'] == False]])\n",
    "  #print(f\"for group duplicates to add has size {len(duplicates_to_add)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(duplicates_to_add.index)} can be re-added to the no-duplicated dataframe\")\n",
    "duplicates_to_add.drop(\"_merge\", axis='columns', inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzer(description, df_plain, df_clean):\n",
    "  print(f\"for {description}:\")\n",
    "  print(f\"size difference is: {len(df_plain.index)-len(df_clean.index)}\")\n",
    "  print(f\"  {len(df_clean.index)} information score cleaned: {df_clean[\"info_score\"].mean()}\")\n",
    "  print(f\"  {len(df_plain.index)} information score dropout: {df_plain[\"info_score\"].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data integrity and make sure duplicates_to_add and beta_df_no_duplicates have no same rows\n",
    "merged_df = pd.merge(beta_df_no_duplicates, duplicates_to_add, on=cultivated_columns, indicator=True, how='inner')\n",
    "\n",
    "# Check if merged_df is empty\n",
    "if not merged_df.empty:\n",
    "  print(merged_df)  # Optional: Display the common rows\n",
    "  print(\"There are identical rows between the two DataFrames.\")\n",
    "  raise Exception(\"ERROR: There shouldn't be identical rows\")\n",
    "\n",
    "final_beta_df = pd.concat([beta_df_no_duplicates, duplicates_to_add])\n",
    "print(f\"from the plain dataset which has {len(beta_df.index)} entries, {len(beta_df.index)-len(final_beta_df.index)} entries have been removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer(\"beta dataset \", beta_df, final_beta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_beta_df[\"TCR_name\"] = range(1, len(final_beta_df) + 1)\n",
    "final_beta_df[\"Binding\"] = 1\n",
    "final_beta_df.drop(\"info_score\", axis='columns', inplace=True)\n",
    "print(f\"final_beta_df length = {len(final_beta_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_beta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_beta_df.to_csv(custom_dataset_path+output_file_beta, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------\n",
    "-------------------------------------------------------------------------------------------------\n",
    "\n",
    "Here we concatenate the paired datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_df = pd.concat([vdjdb_paired_df, mcpastcr_paired_df, iedb_paired_df], axis=0)\n",
    "obsolete_columns = [\"TRA_leader\", \"TRB_leader\", \"Linker\", \"Link_order\", \"TRA_5_prime_seq\", \"TRA_3_prime_seq\", \"TRB_5_prime_seq\", \"TRB_3_prime_seq\",\\\n",
    "                    \"Score\", \"MHC class\"]\n",
    "paired_df = paired_df.drop(columns=obsolete_columns)\n",
    "print(f\"length of paired_df: {len(paired_df.index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before dropping duplicates we need to order the entries based on how many informations they contain\n",
    "paired_df[\"info_score\"] = paired_df.notnull().sum(axis=1)\n",
    "paired_df = paired_df.sort_values(by=['info_score'], ascending=[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_df[\"MHC\"] = paired_df[\"MHC\"].apply(standardize_mhc)\n",
    "paired_df[\"TRAV\"] = paired_df[\"TRAV\"].apply(standardize_vj)\n",
    "paired_df[\"TRBV\"] = paired_df[\"TRBV\"].apply(standardize_vj)\n",
    "paired_df[\"TRAJ\"] = paired_df[\"TRAJ\"].apply(standardize_vj)\n",
    "paired_df[\"TRBJ\"] = paired_df[\"TRBJ\"].apply(standardize_vj)\n",
    "paired_df[\"TRA_CDR3\"] = paired_df[\"TRA_CDR3\"].apply(standardize_cdr3)\n",
    "paired_df[\"TRB_CDR3\"] = paired_df[\"TRB_CDR3\"].apply(standardize_cdr3)\n",
    "paired_df = paired_df.dropna(subset=[\"TRA_CDR3\", \"TRB_CDR3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The following script removes a lot of rows. They are kept and some of them get added again later\")\n",
    "cultivated_columns = paired_df.columns.difference([\"TCR_name\", \"info_score\"]).tolist()\n",
    "most_important_columns = [\"TRA_CDR3\", \"TRB_CDR3\", \"Epitope\"]\n",
    "\n",
    "paired_df_distinct = paired_df.drop_duplicates(subset=cultivated_columns, keep=\"first\")\n",
    "print(f\"distinct entries (all columns, keep=first). {len(paired_df.index)-len(paired_df_distinct.index)} entries removed.\")\n",
    "\n",
    "paired_df_no_duplicates = paired_df_distinct.drop_duplicates(subset=most_important_columns, keep=False)\n",
    "print(f\"removed all duplicates from distinct values (cultivated columns, keep=False). {len(paired_df_distinct.index)-len(paired_df_no_duplicates.index)} entries removed.\")\n",
    "\n",
    "paired_df_removed_entries = pd.merge(paired_df_distinct, paired_df_no_duplicates, how=\"left\", indicator=True)\n",
    "paired_df_removed_entries = paired_df_removed_entries[paired_df_removed_entries['_merge'] == 'left_only'] # left-only values from left-join merge are dropped out rows\n",
    "print(f\"paired removed entries df length: {len(paired_df_removed_entries.index)}\")\n",
    "print()\n",
    "print()\n",
    "\n",
    "# Check data integrity and make sure paired_df_removed_entries and paired_df_no_duplicates have no same rows\n",
    "paired_df_removed_entries.drop(\"_merge\", axis='columns', inplace=True)\n",
    "merged_df = pd.merge(paired_df_removed_entries, paired_df_no_duplicates, on=most_important_columns, indicator=True, how='inner')\n",
    "# Check if merged_df is empty\n",
    "if not merged_df.empty:\n",
    "  print(merged_df)  # Optional: Display the common rows\n",
    "  print(\"There are identical rows between the two DataFrames.\")\n",
    "  raise Exception(\"ERROR: There shouldn't be identical rows\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare both lists\n",
    "paired_df_removed_entries['is_duplicated'] = False\n",
    "duplicates_to_add = pd.DataFrame(columns=paired_df_removed_entries.columns)\n",
    "\n",
    "# Iterate from top to bottom (top has highest information score)\n",
    "paired_df_removed_grouped = paired_df_removed_entries.groupby(most_important_columns)\n",
    "\n",
    "for name, group in paired_df_removed_grouped:\n",
    "  group = group.sort_values(by=['info_score'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "  for i in range(len(group.index)-2):\n",
    "    if group.iloc[i]['is_duplicated'].any(): continue\n",
    "\n",
    "    for j in range(i+1, len(group.index)-1):\n",
    "      if not group.iloc[j]['is_duplicated'].any() and is_duplicate(group.iloc[i], group.iloc[j], cultivated_columns):\n",
    "        group.at[j, 'is_duplicated'] = True\n",
    "    \n",
    "  duplicates_to_add = pd.concat([duplicates_to_add, group[group['is_duplicated'] == False]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "# Now I want to re-add some of the duplicated elements which have the highest info_score.\n",
    "paired_df_removed_grouped = paired_df_removed_entries.groupby(most_important_columns)\n",
    "duplicates_to_add = pd.DataFrame(columns=paired_df_removed_entries.columns) #  create empty dataframe\n",
    "\n",
    "for name, group in paired_df_removed_grouped:\n",
    "    highest_info_score = group['info_score'].max()\n",
    "\n",
    "    for index, row in group.iterrows():\n",
    "      removed = True\n",
    "      if row[\"info_score\"] == highest_info_score:\n",
    "        removed = False\n",
    "        row = pd.DataFrame([row])\n",
    "        duplicates_to_add = pd.concat([duplicates_to_add, row], ignore_index=True)\n",
    "\n",
    "print(f\"{len(duplicates_to_add.index)} will be re-added to the no-duplicates dataframe\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(duplicates_to_add.index)} can be re-added to the no-duplicated dataframe\")\n",
    "duplicates_to_add.drop(\"_merge\", axis='columns', inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data integrity and make sure duplicates_to_add and paired_df_no_duplicates have no same rows\n",
    "merged_df = pd.merge(paired_df_no_duplicates, duplicates_to_add, on=most_important_columns, indicator=True, how='inner')\n",
    "\n",
    "# Check if merged_df is empty\n",
    "if not merged_df.empty:\n",
    "  print(merged_df)  # Optional: Display the common rows\n",
    "  print(\"There are identical rows between the two DataFrames.\")\n",
    "  raise Exception(\"ERROR: There shouldn't be identical rows\")\n",
    "\n",
    "final_paired_df = pd.concat([paired_df_no_duplicates, duplicates_to_add])\n",
    "print(f\"from the plain dataset which has {len(paired_df.index)} entries, {len(paired_df.index)-len(final_paired_df.index)} entries have been removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer(\"paired dataset\", paired_df, final_paired_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_paired_df[\"TCR_name\"] = range(1, len(final_paired_df) + 1)\n",
    "final_paired_df[\"Binding\"] = 1\n",
    "final_paired_df.drop(\"info_score\", axis='columns', inplace=True)\n",
    "print(f\"final_paired_df length: {len(final_paired_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_paired_df.to_csv(custom_dataset_path+output_file_paired, sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BA24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
