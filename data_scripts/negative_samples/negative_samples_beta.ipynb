{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline, T5Tokenizer, T5EncoderModel\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Samples Creation w/ Embeddings and Coisne Similarity\n",
    "\n",
    "Here we chose the approach to create embeddings from the prot_t5_xl model and then check if the embedding is in a certain \"un-similarity range\" which is a self-defined threshold. This becuase evaluations showed that this is (at least as far as we consider) the \"best\" approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 'precision' in locals():\n",
    "  precision = \"gene\" # gene or allele\n",
    "\n",
    "if not 'read_path_train' in locals():\n",
    "  read_path_train = f\"../../data/splitted_data/{precision}/beta/train.tsv\"\n",
    "\n",
    "if not 'read_path_validation' in locals():\n",
    "  read_path_validation = f\"../../data/splitted_data/{precision}/beta/validation.tsv\"\n",
    "\n",
    "if not 'read_path_test' in locals():\n",
    "  read_path_test = f\"../../data/splitted_data/{precision}/beta/test.tsv\"\n",
    "\n",
    "if not 'temp_path' in locals():\n",
    "  temp_path = \"../../data/customDatasets/negative_samples/temp/\"\n",
    "\n",
    "if not 'output_path' in locals():\n",
    "  output_path = f\"../../data/customDatasets/negative_samples/{precision}/\"\n",
    "\n",
    "if not 'train_output_name' in locals():\n",
    "  train_output_name = \"beta_train_concatenated_with_negative.tsv\"\n",
    "\n",
    "if not 'validation_output_name' in locals():\n",
    "  validation_output_name = \"beta_validation_concatenated_with_negative.tsv\"\n",
    "\n",
    "if not 'test_output_name' in locals():\n",
    "  test_output_name = \"beta_test_concatenated_with_negative.tsv\"\n",
    "\n",
    "beta_train_df = pd.read_csv(read_path_train, sep=\"\\t\")\n",
    "beta_validation_df = pd.read_csv(read_path_validation, sep=\"\\t\")\n",
    "beta_test_df = pd.read_csv(read_path_test, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Load encoder-part of ProtT5 in half-precision. { display-mode: \"form\" }\n",
    "# Load ProtT5 in half-precision (more specifically: the encoder-part of ProtT5-XL-U50 in half-precision)\n",
    "transformer_link = \"Rostlab/prot_t5_xl_half_uniref50-enc\"\n",
    "print(\"Loading: {}\".format(transformer_link))\n",
    "model = T5EncoderModel.from_pretrained(transformer_link)\n",
    "if device==torch.device(\"cpu\"):\n",
    "  print(\"Casting model to full precision for running on CPU ...\")\n",
    "  model.to(torch.float32) # only cast to full-precision if no GPU is available\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "tokenizer = T5Tokenizer.from_pretrained(transformer_link, do_lower_case=False, legacy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epitopes_train = set(beta_train_df[\"Epitope\"].to_list())\n",
    "epitopes_validation = set(beta_validation_df[\"Epitope\"].to_list())\n",
    "epitopes_test = set(beta_test_df[\"Epitope\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will replace all rare/ambiguous amino acids by X and introduce white-space between all amino acids\n",
    "processed_train_epitopes = [(sequence, \" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence)))) for sequence in epitopes_train]\n",
    "processed_validation_epitopes = [(sequence, \" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence)))) for sequence in epitopes_validation]\n",
    "processed_test_epitopes = [(sequence, \" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence)))) for sequence in epitopes_test]\n",
    "# processed_epitopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(processed_seqs):\n",
    "    # Extract just the processed sequences for tokenization\n",
    "    sequences = [seq[1] for seq in processed_seqs]\n",
    "    ids = tokenizer.batch_encode_plus(sequences, add_special_tokens=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "    input_ids = ids['input_ids'].to(device)\n",
    "    attention_mask = ids['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    \n",
    "    # Now, return embeddings mapped to the original sequence\n",
    "    embeddings = {}\n",
    "    for i, (original_seq, _) in enumerate(processed_seqs):\n",
    "        seq_len = len(original_seq)\n",
    "        valid_embeddings = last_hidden_states[i,:seq_len]\n",
    "        per_protein_embedding = valid_embeddings.mean(dim=0)        \n",
    "        embedding = per_protein_embedding.cpu().numpy()\n",
    "        embeddings[original_seq] = embedding  # Use original sequence as key\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_train_name = \"negative_train_samples_beta_embeddings_dict.npz\"\n",
    "file_validation_name = \"negative_validation_samples_beta_embeddings_dict.npz\"\n",
    "file_test_name = \"negative_test_samples_beta_embeddings_dict.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 128\n",
    "sequence_train_to_embedding = {}\n",
    "sequence_validation_to_embedding = {}\n",
    "sequence_test_to_embedding = {}\n",
    "\n",
    "# Batch processing with a dictionary, using original sequences as keys\n",
    "for i in range(0, len(processed_train_epitopes), batch_size):\n",
    "    batch_sequences = processed_train_epitopes[i:i+batch_size]\n",
    "    batch_embeddings = process_batch(batch_sequences)\n",
    "    sequence_train_to_embedding.update(batch_embeddings)\n",
    "\n",
    "for i in range(0, len(processed_validation_epitopes), batch_size):\n",
    "    batch_sequences = processed_validation_epitopes[i:i+batch_size]\n",
    "    batch_embeddings = process_batch(batch_sequences)\n",
    "    sequence_validation_to_embedding.update(batch_embeddings)\n",
    "\n",
    "for i in range(0, len(processed_test_epitopes), batch_size):\n",
    "    batch_sequences = processed_test_epitopes[i:i+batch_size]\n",
    "    batch_embeddings = process_batch(batch_sequences)\n",
    "    sequence_test_to_embedding.update(batch_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(temp_path+file_train_name, **sequence_train_to_embedding)\n",
    "np.savez(temp_path+file_validation_name, **sequence_validation_to_embedding)\n",
    "np.savez(temp_path+file_test_name, **sequence_test_to_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epitope_train_to_embedding = np.load(temp_path+file_train_name)\n",
    "epitope_validation_to_embedding = np.load(temp_path+file_validation_name)\n",
    "epitope_test_to_embedding = np.load(temp_path+file_test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_index = len(beta_train_df) - 1 \n",
    "max_validation_index = len(beta_validation_df) - 1 \n",
    "max_test_index = len(beta_test_df) - 1 \n",
    "negative_train_epitopes_cosine = []\n",
    "negative_validation_epitopes_cosine = []\n",
    "negative_test_epitopes_cosine = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(embedding1, embedding2): \n",
    "    cosine = np.dot(embedding1,embedding2)/(np.linalg.norm(embedding1)*np.linalg.norm(embedding2))\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_negative(cosine_similarity, current_epitope, random_epitope): \n",
    "    is_valid = False\n",
    "    cosine_min = -1\n",
    "    cosine_max = 0.75\n",
    "\n",
    "    if (cosine_similarity >= cosine_min \\\n",
    "        and cosine_similarity <= cosine_max) \\\n",
    "        and (current_epitope != random_epitope): \n",
    "        is_valid = True \n",
    "\n",
    "    return is_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_max_depth = sys.getrecursionlimit()\n",
    "max_attempts_by_system = sys_max_depth - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_negative_epitope_embedding(df, index, current_epitope, epitope_to_embedding, max_attempts=max_attempts_by_system): \n",
    "    current_epitope = df[\"Epitope\"][index]\n",
    "    current_embedding = epitope_to_embedding[current_epitope]\n",
    "    attempt = 0\n",
    "    \n",
    "    while attempt < max_attempts:\n",
    "        random_epitope_index = np.random.randint(0, len(df))\n",
    "        random_epitope = df[\"Epitope\"][random_epitope_index]\n",
    "        random_mhc = df[\"MHC\"][random_epitope_index]\n",
    "        \n",
    "        if random_epitope_index == index:\n",
    "            attempt += 1\n",
    "            continue  # Skip the rest of the loop and try again\n",
    "        \n",
    "        random_epitope_embedding = epitope_to_embedding[random_epitope]\n",
    "        cosine = cosine_similarity(current_embedding, random_epitope_embedding)\n",
    "        \n",
    "        if is_valid_negative(cosine, current_epitope, random_epitope) or attempt == max_attempts - 1:\n",
    "            return (random_epitope, random_mhc)  # Return the found valid or last attempt epitope\n",
    "        \n",
    "        attempt += 1\n",
    "    \n",
    "    # This point should theoretically never be reached because of the check in the loop,\n",
    "    # but it's a fallback to return a random different epitope if for some reason it does.\n",
    "    while True:\n",
    "        random_epitope_index = np.random.randint(0, len(df))\n",
    "        if random_epitope_index != index:\n",
    "            return df[\"Epitope\"][random_epitope_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, epitope in enumerate(beta_train_df[\"Epitope\"]):\n",
    "    negative_train_epitope = search_negative_epitope_embedding(beta_train_df, i, epitope, epitope_train_to_embedding)\n",
    "    negative_train_epitopes_cosine.append(negative_train_epitope)\n",
    "\n",
    "for i, epitope in enumerate(beta_validation_df[\"Epitope\"]):\n",
    "    negative_validation_epitope = search_negative_epitope_embedding(beta_validation_df, i, epitope, epitope_validation_to_embedding)\n",
    "    negative_validation_epitopes_cosine.append(negative_validation_epitope)\n",
    "\n",
    "for i, epitope in enumerate(beta_test_df[\"Epitope\"]):\n",
    "    negative_test_epitope = search_negative_epitope_embedding(beta_test_df, i, epitope, epitope_test_to_embedding)\n",
    "    negative_test_epitopes_cosine.append(negative_test_epitope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epitopes_train = []\n",
    "epitopes_validation = []\n",
    "epitopes_test = []\n",
    "mhc_train = []\n",
    "mhc_validation = []\n",
    "mhc_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row_infos in negative_train_epitopes_cosine:\n",
    "    epitopes_train.append(row_infos[0]) \n",
    "    mhc_train.append(row_infos[1])\n",
    "    \n",
    "for row_infos in negative_validation_epitopes_cosine:\n",
    "    epitopes_validation.append(row_infos[0]) \n",
    "    mhc_validation.append(row_infos[1])\n",
    "\n",
    "for row_infos in negative_test_epitopes_cosine:\n",
    "    epitopes_test.append(row_infos[0]) \n",
    "    mhc_test.append(row_infos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_train_epitopes_cosine_dict = {\"Negative Epitope\": epitopes_train, \"MHC\": mhc_train}\n",
    "negative_train_epitopes_cosine_df = pd.DataFrame(negative_train_epitopes_cosine_dict)\n",
    "\n",
    "negative_validation_epitopes_cosine_dict = {\"Negative Epitope\": epitopes_validation, \"MHC\": mhc_validation}\n",
    "negative_validation_epitopes_cosine_df = pd.DataFrame(negative_validation_epitopes_cosine_dict)\n",
    "\n",
    "negative_test_epitopes_cosine_dict = {\"Negative Epitope\": epitopes_test, \"MHC\": mhc_test}\n",
    "negative_test_epitopes_cosine_df = pd.DataFrame(negative_test_epitopes_cosine_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_train_negative_epitope_df = beta_train_df.drop([\"MHC\"], axis=1).copy(deep=True)\n",
    "beta_train_negative_epitope_df[\"Epitope\"] = epitopes_train\n",
    "beta_train_negative_epitope_df[\"MHC\"] = mhc_train\n",
    "beta_train_negative_epitope_df[\"Binding\"] = 0\n",
    "beta_train_negative_epitope_df\n",
    "\n",
    "beta_validation_negative_epitope_df = beta_validation_df.drop([\"MHC\"], axis=1).copy(deep=True)\n",
    "beta_validation_negative_epitope_df[\"Epitope\"] = epitopes_validation\n",
    "beta_validation_negative_epitope_df[\"MHC\"] = mhc_validation\n",
    "beta_validation_negative_epitope_df[\"Binding\"] = 0\n",
    "beta_validation_negative_epitope_df\n",
    "\n",
    "beta_test_negative_epitope_df = beta_test_df.drop([\"MHC\"], axis=1).copy(deep=True)\n",
    "beta_test_negative_epitope_df[\"Epitope\"] = epitopes_test\n",
    "beta_test_negative_epitope_df[\"MHC\"] = mhc_test\n",
    "beta_test_negative_epitope_df[\"Binding\"] = 0\n",
    "beta_test_negative_epitope_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_train_with_negative_df = pd.concat([beta_train_df.copy(deep=True), beta_train_negative_epitope_df], axis=0)\n",
    "beta_train_with_negative_df\n",
    "\n",
    "beta_validation_with_negative_df = pd.concat([beta_validation_df.copy(deep=True), beta_validation_negative_epitope_df], axis=0)\n",
    "beta_validation_with_negative_df\n",
    "\n",
    "beta_test_with_negative_df = pd.concat([beta_test_df.copy(deep=True), beta_test_negative_epitope_df], axis=0)\n",
    "beta_test_with_negative_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_ignore_for_duplicates = beta_train_with_negative_df.columns.difference([\"TCR_name\", \"Binding\"])\n",
    "beta_train_with_negative_df.drop_duplicates(inplace=True, subset=columns_to_ignore_for_duplicates, keep=\"first\")\n",
    "beta_train_with_negative_df[\"TCR_name\"] = range(1, len(beta_train_with_negative_df)+1)\n",
    "beta_train_with_negative_df.reset_index(drop=True, inplace=True)\n",
    "beta_train_with_negative_df\n",
    "\n",
    "columns_to_ignore_for_duplicates = beta_validation_with_negative_df.columns.difference([\"TCR_name\", \"Binding\"])\n",
    "beta_validation_with_negative_df.drop_duplicates(inplace=True, subset=columns_to_ignore_for_duplicates, keep=\"first\")\n",
    "beta_validation_with_negative_df[\"TCR_name\"] = range(1, len(beta_validation_with_negative_df)+1)\n",
    "beta_validation_with_negative_df.reset_index(drop=True, inplace=True)\n",
    "beta_validation_with_negative_df\n",
    "\n",
    "columns_to_ignore_for_duplicates = beta_test_with_negative_df.columns.difference([\"TCR_name\", \"Binding\"])\n",
    "beta_test_with_negative_df.drop_duplicates(inplace=True, subset=columns_to_ignore_for_duplicates, keep=\"first\")\n",
    "beta_test_with_negative_df[\"TCR_name\"] = range(1, len(beta_test_with_negative_df)+1)\n",
    "beta_test_with_negative_df.reset_index(drop=True, inplace=True)\n",
    "beta_test_with_negative_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_train_with_negative_df.to_csv(output_path+\"/\"+train_output_name, sep=\"\\t\", index=False)\n",
    "beta_validation_with_negative_df.to_csv(output_path+\"/\"+validation_output_name, sep=\"\\t\", index=False)\n",
    "beta_test_with_negative_df.to_csv(output_path+\"/\"+test_output_name, sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BA24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
